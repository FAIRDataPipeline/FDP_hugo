<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Introduction on FAIRDataPipeline</title>
    <link>https://www.fairdatapipeline.org/</link>
    <description>Recent content in Introduction on FAIRDataPipeline</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <atom:link href="https://www.fairdatapipeline.org/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://www.fairdatapipeline.org/docs/API/python/standard_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.fairdatapipeline.org/docs/API/python/standard_api/</guid>
      <description>Standardised data type API # Unless otherwise specified this document uses the “current” metadata scheme defined in the &amp;ldquo;Metadata and data representation” document. You do not need to have read that document to be able to read and understand (the majority of) this one.
The standardised data type API is defined more in terms of file formats than it is in terms of data types. There are two file formats: parameter files, and hdf5 files.</description>
    </item>
    <item>
      <title>Installation Instructions</title>
      <link>https://www.fairdatapipeline.org/docs/data_registry/installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.fairdatapipeline.org/docs/data_registry/installation/</guid>
      <description>Local FAIR data registry # The documentation for the registry is available here, and is the same for the local and remote registry.
Installation # There are a few alternative ways to install a local FAIR data registry and we describe the different options below.
Dependencies # The registry relies on the graphviz package to produce the schema visualisation and the provenance report, so you will need to follow graphviz installation process for your system before initialising a local registry.</description>
    </item>
    <item>
      <title></title>
      <link>https://www.fairdatapipeline.org/docs/API/python/file_api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.fairdatapipeline.org/docs/API/python/file_api/</guid>
      <description>File API # The file API manages file access, provenance, and metadata # The API is accessed as a &amp;ldquo;session&amp;rdquo;. All reads and writes are recorded and logged into a file when the session closes. Files are identified by their metadata, though the metadata is handled differently for reads (where the files are expected to exist) and writes (where they typically do not), described in more detail below.
The file API behaviour is entirely determined by a yaml configuration file (referred to here as a “config.</description>
    </item>
    <item>
      <title>Schema Description</title>
      <link>https://www.fairdatapipeline.org/docs/data_registry/schema/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.fairdatapipeline.org/docs/data_registry/schema/</guid>
      <description>The FAIR data registry schema has been designed to address the different requirements around the information needed on different research objects associated with an epidemiological pipeline.
Main entities # The main entities represented in the schema are around the data and the software for epidemiological models:
DataProduct: A dataset that is used by or is generated by a model. ExternalObject: An external data object, i.e. one that has comes from somewhere other than being generated as part of the modelling pipeline.</description>
    </item>
    <item>
      <title>Provenance Report</title>
      <link>https://www.fairdatapipeline.org/docs/data_registry/prov_report/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.fairdatapipeline.org/docs/data_registry/prov_report/</guid>
      <description>Provenance Report # In order to address the use case around being able to track the evidence to understand the reported results, the data registry has the capability to produce provenance reports for each of the data products.
Provenance is the documented history of processes in a digital object&amp;rsquo;s lifecycle.
The provenance reports generated by the registry are based around the concepts of activities, agents and entities. For more information about these concepts see the PROV Ontology or PROV-O.</description>
    </item>
    <item>
      <title>Terminology</title>
      <link>https://www.fairdatapipeline.org/docs/API/python/terminology/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.fairdatapipeline.org/docs/API/python/terminology/</guid>
      <description>Terminology used in this document # datum A specific value, encoded in a particular way, that travels through the data pipeline. config.yaml A file (potentially with a different name) used by the data pipeline to allow users to override default pipeline behaviour. See the File API specification for more details. metadata.yaml A file used by the data pipeline to describe available data files, listing their associated metadata. See the File API specification for more details.</description>
    </item>
    <item>
      <title>RO Crate Objects</title>
      <link>https://www.fairdatapipeline.org/docs/data_registry/ro_crate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.fairdatapipeline.org/docs/data_registry/ro_crate/</guid>
      <description> Research Object Crate (RO-Crate) # </description>
    </item>
    <item>
      <title></title>
      <link>https://www.fairdatapipeline.org/docs/API/Java/coderun/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.fairdatapipeline.org/docs/API/Java/coderun/</guid>
      <description>javaDataPipeline Coderun # The main class used to interface with the FAIR DataPipeline in Java, is the Coderun class.
Users should initialise the Coderun instance using a try-with-resources block or ensure that .close() is explicitly called upon finishing.
The Coderun constructor needs a Path to the config.yaml, a Path to the script.sh, and the registry authentication token.
The user then would access the input data product(s) using coderun.get_dp_for_read(dataproduct_name) and output data product(s) using coderun.</description>
    </item>
    <item>
      <title></title>
      <link>https://www.fairdatapipeline.org/docs/API/Java/issues/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.fairdatapipeline.org/docs/API/Java/issues/</guid>
      <description>javaDataPipeline Issues # Issues can be attached to Object_components, as well as to the Submission Script, Config File, and Code Repo.
Issues can be created and then linked to the elements that they apply to, or they can be raised directly on the element it refers to.
Here is an example of the creation then linking of an Issue to components, Script, and Code Repo:
try (Coderun coderun = new Coderun(configPath, scriptPath, token)) { Data_product_read dp = coderun.</description>
    </item>
    <item>
      <title></title>
      <link>https://www.fairdatapipeline.org/docs/API/Java/parameters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.fairdatapipeline.org/docs/API/Java/parameters/</guid>
      <description>Parameters # Below are examples of all the parameter types that can be written to and read from the FAIR Data Pipeline TOML parameter file format. These TOML parameter files are described on the API page.
Samples
samples = ImmutableSamples.builder().addSamples(1, 2, 3).rng(rng).build(); object_component.writeSamples(samples); Distribution (Gamma)
distribution = ImmutableDistribution.builder() .internalShape(1) .internalScale(2) .internalType(DistributionType.gamma) .rng(rng) .build(); object_component.writeDistribution(distribution); Categorical Distribution
MinMax firstMinMax = ImmutableMinMax.builder() .isLowerInclusive(true) .isUpperInclusive(true) .lowerBoundary(0) .upperBoundary(4) .build(); MinMax secondMinMax = ImmutableMinMax.builder() .</description>
    </item>
    <item>
      <title></title>
      <link>https://www.fairdatapipeline.org/docs/API/Java/hdf5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.fairdatapipeline.org/docs/API/Java/hdf5/</guid>
      <description>HDF5 # HDF files are not supported in the current version of the Java FAIR Data Pipeline API.</description>
    </item>
    <item>
      <title></title>
      <link>https://www.fairdatapipeline.org/docs/API/Java/run/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.fairdatapipeline.org/docs/API/Java/run/</guid>
      <description>Running your code # Below is a brief explanation how to make use of the Java FAIRDataPipeline library and actually getting your code to run from the FAIR CLI command line interface. The actual command to run your Java code is given in the script: line in the config.yaml. In my case I run the Java code using gradle run --args &amp;quot;${{CONFIG_DIR}}&amp;quot;. You&amp;rsquo;ll have to change this if you don&amp;rsquo;t use gradle.</description>
    </item>
    <item>
      <title></title>
      <link>https://www.fairdatapipeline.org/docs/API/Java/debug/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://www.fairdatapipeline.org/docs/API/Java/debug/</guid>
      <description>Debugging / troubleshooting # Logger # The javaDataPipeline library uses SLF4J (Simple Logging Facade for Java).
You can either use this with the SLF4J simple logger, or it can bind to your own favourite logging framework. In order to set the default simple logger to more verbose logging:
add the org.slf4j:slf4j-simple dependency set the log level: System.setProperty(org.slf4j.impl.SimpleLogger.DEFAULT_LOG_LEVEL_KEY, &amp;quot;TRACE&amp;quot;);
Running manually # Instead of running:
fair run src/main/resources/config.yaml You can call fair run and your actual java code separately:</description>
    </item>
  </channel>
</rss>
