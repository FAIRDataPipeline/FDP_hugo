{"1": {
        "url": "/docs/API/python/standard_api/",
        "relUrl": "/docs/API/python/standard_api/",
        "title": "Standard API",
        "doc": "Standard API",
        "section": "Python",
        "content": " Standardised data type API # Unless otherwise specified this document uses the “current” metadata scheme defined in the \u0026ldquo;Metadata and data representation” document. You do not need to have read that document to be able to read and understand (the majority of) this one.\nThe standardised data type API is defined more in terms of file formats than it is in terms of data types. There are two file formats: parameter files, and hdf5 files. Parameter files store “small” parameter data in toml format, representing individual parameters. HDF5 files are used to store structured data, encoded either as “arrays” or “tables”. Both formats are described in more detail below, alongside API functions used to interact with them.\nInitialisation # The standard API must be initialised with the model URI and git sha, which should then be set as run-level metadata using the file API set_metadata function.\nAdditional metadata on write # The write functions all accept description and issues arguments, which are passed through to the file API as component-level metadata.\nparameter files # A parameter file contains representations of one or more parameters, each a single number, possibly with some associated uncertainty. Parameters may by represented as point-estimates, parametric distributions, and sample data.\nMetadata # extension = \u0026#34;toml\u0026#34; Alternative metadata # format = \u0026#34;parameter\u0026#34; extension = \u0026#34;toml\u0026#34; (could be inferred) File format # Parameters are stored in toml-formatted files, with the extension “toml”, containing sections corresponding to different components. The following is an example of the internal encoding, defining three components: \u0026ldquo;my-point-estimate\u0026rdquo;, \u0026ldquo;my-distribution\u0026rdquo;, and \u0026ldquo;my-samples\u0026rdquo;:\n[my-point-estimate] type = \u0026#34;point-estimate\u0026#34; value = 0.1 [my-distribution] type = \u0026#34;distribution\u0026#34; distribution = \u0026#34;gamma\u0026#34; shape = 1 scale = 2 [my-samples] type = \u0026#34;samples\u0026#34; samples = [1.0, 2.0, 3.0, 4.0, 5.0] Point estimates are used when our knowledge of the parameter is only sufficient for a single value, with no notion of uncertainty. A point estimate component must have type = \u0026ldquo;point-estimate\u0026rdquo; and a value that is either a float or an integer.\nDistributions are used when our knowledge of a parameter can be represented by a parametric distribution. A distribution component must have type = \u0026ldquo;distribution\u0026rdquo;, a distribution set to a string name of the distribution, and other parameters determined by the distribution. The set of distributions required to be supported is currently undefined.\nSamples are used when our knowledge of a parameter is represented by samples, from either empirical measurements, or a posterior distribution. A samples component must have type = \u0026ldquo;samples\u0026rdquo; and a value that is a list of floats and integers.\nDistributions # The supported distributions,each with a link to information about their parameterisation, and their standardised parameter names are as follows:\nDistribution Standardised parameter names categorical (non-standard) bins (string array), weights (float array) gamma k (float), theta (float) normal mu (float), sigma (float) uniform a (float), b (float) poisson lambda (float) exponential lambda (float) beta alpha (float), beta (float) binomial n (int), p (float) multinomial n (int), p (float array) API functions # read_estimate(data_product, component) -\u0026gt; float or integer\nIf the component is represented as a point estimate, return that value.\nIf the component is represented as a distribution, return the distribution mean.\nIf the component is represented as samples, return the sample mean.\nread_distribution(data_product, component) -\u0026gt; distribution object\nIf the component is represented as a point estimate, fail.\nIf the component is represented as a distribution, return an object representing that distribution.\nIf the component is represented as samples, failreturn an empirical distribution.\nread_samples(data_product, component) -\u0026gt; list of floats or integers\nIf the component is represented as a point estimate, fail.\nIf the component is represented as a distribution, fail.\nIf the component is represented as samples, return the samples.\nwrite_estimate(data_product, component, estimate, description, issues)\nwrite_distribution(data_product, component, distribution object, description, issues)\nwrite_samples(data_product, component, samples, description, issues)\nHDF5 files # HDF5 files contain structured data, encoded as either an “array”, or a “table”, both of which are described in more detail below.\nMetadata # extension = \u0026ldquo;h5\u0026rdquo;\nAlternative metadata # format = \u0026ldquo;hdf5\u0026rdquo;\nextension = \u0026ldquo;h5\u0026rdquo; (could be inferred)\nFile format # HDF5 files are stored with the extension “h5”. Internally, each component is stored in a different (possibly nested) group, where the full path defines the component name (e.g. “path/to/component”). Inside the group for each component is either a value named “array”, or a value named “table”. It is an error for there to be both.\narray format # {component}/array An n-dimensional array of numerical data {component}/Dimension_{i}_title The string name of dimension \\(i\\) {component}/Dimension_{i}_names String labels for dimension \\(i\\) {component}/Dimension_{i}_values Values for dimension \\(i\\) {component}/Dimension_{i}_units Units for dimension \\(i\\) {component}/units Units for the data in array table format # {component}/table A dataframe {component}/row_names String labels for the row axis {component}/column_units Units for the columns API functions # read_array(data_product, component) -\u0026gt; array\nIf the component does not terminate in an array-formatted value, raise an error.\nReturn an array, currently with no structural information.\nread_table(data_product, component) -\u0026gt; dataframe\nIf the component does not terminate in a table-formatted value, raise an error.\nReturn a dataframe, with labelled columns.\nwrite_array(data_product, component, array, description, issues)\nwrite_tab\n"
      }
      ,"2": {
        "url": "/docs/cli/cli_readme/",
        "relUrl": "/docs/cli/cli_readme/",
        "title": "FAIR CLI readme",
        "doc": "FAIR CLI readme",
        "section": "Fair CLI",
        "content": " fair CLI outline # DISCLAIMER: The following document is largely conceptual and therefore does not represent a manual for the final interface. Statements within the following are likely to change, further details of possible changes are given throughout. Please either open an issue or pull request on the source repository raising any changes/issues. FAIR-CLI forms the main interface for synchronising changes between your local and shared remote FAIR Data Pipeline registries, it is also used to instantiate model runs/data submissions to the pipeline.\nThe project is still under development with many features still to be implemented and checked. Available commands are summarised below along with their usage.\nInstallation # The project makes use of Poetry for development which allows quick and easy mangement of dependencies, and provides a virtual environment exclusive to the project. Ultimately the project will be built into a pip installable module (using poetry build) meaning users will not need Poetry. You can access this environment by installing poetry:\npip install poetry and, ensuring you are in the project repository, running:\npoetry install which will setup the virtual environment and install requirements. You can then either launch the environment as a shell using:\npoetry shell or run commands within it externally using:\npoetry run \u0026lt;command\u0026gt; Structure # The layout of FAIR-CLI on a simplified system looks like this:\n$HOME ├── .fair │ ├── cli │ │ ├── cli-config.yaml │ │ └── sessions │ ├── data │ │ └── jobs │ └── $REGISTRY_HOME │ └─ Documents └─ my_project ├── config.yaml └── .fair ├── cli-config.yaml ├── logs └── staging Global and Local Directories # FAIR-CLI stores information for projects in two locations. The first is a global directory stored in the user\u0026rsquo;s home folder in the same location as the registry itself $HOME/.fair/cli, and the second is a local directory which exists within the model project itself $PROJECT_HOME/.fair.\nThe CLI holds metadata for the user in it\u0026rsquo;s own configuration file (not to be confused with the user modifiable config.yaml), cli-config.yaml, the global version of which is initialised during first use. In a manner similar to git, FAIR-CLI has repositories which allow the user to override these global configurations, this then forming a local variant.\nData Directory # The directory $HOME/.fair/data is the default data store initialised by FAIR-CLI. During setup an alternative can be provided and this can be later changed on a per-run basis if the user so desires. The subdirectory $HOME/data/jobs contains timestamped directories of jobs.\nSessions Directory # The directory $HOME/.fair/sessions is used to keep track of ongoing queries to the registry as a safety mechanism to ensure the registry is not shutdown whilst processes are still occuring.\nLogs Directory # The directory $PROJECT/.fair/logs stores stdout logs for jobs also giving information on who launched the job and how long it lasted.\nStaging File # The staging file, $PROJECT/.fair/staging, contains information of what jobs are being tracked, by default all jobs are added to this file after completion and are set to \u0026ldquo;unstaged\u0026rdquo;. Simply contains a dictionary of booleans where items for sync (staged) are marked true True and those to be held only locally False. The file uses paths relative to the local .fair folder as keys, to behave in a manner identical to git staging.\nconfig.yaml # This is the main file the user will interact with to customise their run. FAIR-CLI automatically generates a starter version of this file with everything in place. The only addition required is setting of either script or script_path (with the exception of running using fair run bash - see below) under run_metadata.\nscript This should be a command callable by a shell for running a model/submitting data to the registry. This script is saved to a file prior to execution. script_path This is a direct path to an existing script to use for submission. By default the shell used will be sh or pwsh for UNIX and Windows systems respectively, however this can be overwritten with the optional shell key which recognises the following values (where {0} is the script file):\nShell Command bash bash -eo pipefail {0} java java {0} julia julia {0} powershell powershell -command \u0026quot;. '{0}'\u0026quot; pwsh pwsh -command \u0026quot;. '{0}'\u0026quot; python2 python2 {0} python3 python3 {0} python python {0} R R -f {0} sh sh -e {0} NOTE This layout is subject to possible change depending on whether or not multiple aliases for the same user will be allowed in the registry itself. The main reason for having a local version is to support separate handling of multiple projects. Registry Interaction # Currently FAIR-CLI sets up the write data storage location on the local registry if it does not exist. Entries are created for the YAML file type, current user as an author, and object for a given run.\nCommand Line Usage # As mentioned, all of the subcommands within FAIR-CLI are still under review with many still serving as placeholders for future features. Running fair without arguments or fair --help will show all of these.\ninit # Initialises a new FAIR repository within the given directory. This should ideally be the same location as the .git folder for the current project, although setup will ask if you want to use an alternative location. The command will ask the user a series of questions which will provide metadata for tracking run authors, and also allow for the creation of a starter config.yaml.\nThe first time this command is launched the global CLI configuration will be populated. In subsequent calls the global will provide default suggestions towards creating the CLI configuration for the repository (local).\nA repository directory matching the structure above will be placed in the current location and a starter config.yaml file will be generated (see below).\nExample: First call to fair init\nThis example shows the process of setting up for the first time. Note the default suggestions for each prompt, in the case of Full name and Default output namespace this is the hostname of the system and an abbreviated version of this name.\n$ fair init Initialising FAIR repository, setup will now ask for basic info: Checking for local registry Local registry found Remote API URL: http://data.fairdatapipeline.org/api/ Remote Data Storage Root [http://data.fairdatapipeline.org/data/]: Remote API Token File: $HOME/scrc_token.txt Local API URL [http://localhost:8000/api/]: Local registry is offline, would you like to start it? [y/N]: y Default Data Store: [/home/joebloggs/.fair/data]: Email: jbloggs@noreply.uk ORCID [None]: Full Name: Joe Bloggs Default input namespace [None]: SCRC Default output namespace [jbloggs]: Project description: Test project Local Git repository [/home/joebloggs/Documents/AnalysisProject]: Git remote name [origin]: Using git repository remote \u0026#39;origin\u0026#39;: git@notagit.com:jbloggs/AnalysisProject.git Initialised empty fair repository in /home/joebloggs/Documents/AnalysisProject/.fair Example: Subsequent runs\nIn subsequent runs the first time setup will provide further defaults.\n$ fair init Initialising FAIR repository, setup will now ask for basic info: Project description: Test Project Local Git repository [/home/joebloggs/Documents/AnalysisProject]: Git remote name [origin]: Using git repository remote \u0026#39;origin\u0026#39;: git@nogit.com:joebloggs/AnalysisProject.git Remote API URL [http://data.fairdatapipeline.org/api/]: Remote API Token File [/home/kristian/scrc_token.txt]: Local API URL [http://localhost:8000/api/]: Default output namespace [jbloggs]: Default input namespace [SCRC]: Initialised empty fair repository in /home/joebloggs/Documents/AnalysisProject/.fair Generated config.yaml\nrun_metadata: default_input_namespace: SCRC default_output_namespace: jbloggs description: Test Project local_data_registry: http://localhost:8000/api/ local_repo: /home/joebloggs/Documents/AnalysisProject write_data_store: /home/joebloggs/.fair/data the user then only needs to add a script or script_path entry to execute a code run. This is only required for run.\nrun # The purpose of run is to execute a model/submission run to the local registry. The command fills any specified template variables of the form ${{ VAR }} to match those outlined below. Outputs of a run will be stored within the coderun folder in the directory specified under the data_store tag in the config.yaml, by default this is $HOME/.fair/data/coderun.\nfair run If you wish to use an alternative config.yaml then specify it as an additional argument:\nfair run /path/to/config.yaml You can also launch a bash command directly which will then be automatically written into the config.yaml for you:\nfair run --script \u0026#34;echo \\\u0026#34;Hello World\\\u0026#34;\u0026#34; note the command itself must be quoted as it is a single argument.\npull # Currently pull will update any entries within the config.yaml under the register heading creating external_object and data_product objects on the registry and downloading the data to the local data storage. For example:\nrun_metadata: default_input_namespace: SCRC default_output_namespace: jbloggs description: Test project local_data_registry: http://localhost:8000/api/ local_repo: /home/joebloggs/Documents/SCRC/FAIR-CLI write_data_store: /home/joebloggs/.fair/data register: - external_object: records/SARS-CoV-2/scotland/human-mortality namespace_name: Scottish Government Open Data Repository namespace_full_name: Scottish Government Open Data Repository namespace_website: https://statistics.gov.scot/ root: https://statistics.gov.scot/sparql.csv?query= path: |- PREFIX qb: \u0026lt;http://purl.org/linked-data/cube#\u0026gt; PREFIX data: \u0026lt;http://statistics.gov.scot/data/\u0026gt; PREFIX rdfs: \u0026lt;http://www.w3.org/2000/01/rdf-schema#\u0026gt; PREFIX dim: \u0026lt;http://purl.org/linked-data/sdmx/2009/dimension#\u0026gt; PREFIX sdim: \u0026lt;http://statistics.gov.scot/def/dimension/\u0026gt; PREFIX stat: \u0026lt;http://statistics.data.gov.uk/def/statistical-entity#\u0026gt; PREFIX mp: \u0026lt;http://statistics.gov.scot/def/measure-properties/\u0026gt; SELECT ?featurecode ?featurename ?areatypename ?date ?cause ?location ?gender ?age ?type ?count WHERE { ?indicator qb:dataSet data:deaths-involving-coronavirus-covid-19; mp:count ?count; qb:measureType ?measType; sdim:age ?value; sdim:causeOfDeath ?causeDeath; sdim:locationOfDeath ?locDeath; sdim:sex ?sex; dim:refArea ?featurecode; dim:refPeriod ?period. ?measType rdfs:label ?type. ?value rdfs:label ?age. ?causeDeath rdfs:label ?cause. ?locDeath rdfs:label ?location. ?sex rdfs:label ?gender. ?featurecode stat:code ?areatype; rdfs:label ?featurename. ?areatype rdfs:label ?areatypename. ?period rdfs:label ?date. } title: Deaths involving COVID19 description: Nice description of the dataset unique_name: Scottish deaths involving COVID19 file_type: csv release_date: ${{DATETIME}} version: 0.${{DATE}}.0 primary: True if run on 10/10/2021 would download the data from the given root/path URL and store in a file:\n/home/joebloggs/.fair/data/records/SARS-CoV-2/scotland/human-mortality/0.20211010.0.csv and register all required objects into the local registry.\npurge # Removes the local .fair (FAIR repository) folder by default so the user can reinitialise:\nfair purge You can remove the global configuration and start again entirely by running:\nfair purge --glob you will be asked if you wish to erase the data store, do not do this unless you intend on reinstalling the registry itself.\nregistry # By default the CLI will launch the registry whenever a synchronisation or run is called. The server will only be halted once all ongoing CLI processes (in the case of multiple parallel calls) have been completed.\nHowever the user may also specify a manual launch that will override this behaviour, instead leaving the server running constantly allowing them to view the registry in the browser.\nThe commands:\nfair registry start and\nfair registry stop will launch and halt the server respectively.\nlog # Runs are logged locally within the local FAIR repository. A full list of runs is shown by running:\nfair log This will present a list of runs in a summary analogous to a git log call:\nrun 0db35c20946a1ebeaafdc3b30103cd74a57eb6b6 Author: Joe Bloggs \u0026lt;jbloggs@noreply.uk\u0026gt; Date: Wed Jun 30 09:09:30 2021 NOTE The SHA for a job is not yet related to a registry code run identifier as multiple code runs can be executed within a single job. view # To view the stdout of a run given its SHA as shown by running fair log use the command:\nfair view \u0026lt;sha\u0026gt; you do not need to specify the full SHA but rather the first few characters:\n-------------------------------- Commenced = Wed Jun 30 09:09:30 2021 Author = Joe Bloggs \u0026lt;jbloggs@noreply.uk\u0026gt; Namespace = jbloggs Command = bash -eo pipefail /home/jbloggs/.fair/data/coderun/2021-06-30_09_09_30_721358/run_script -------------------------------- 0 1 2 3 4 5 6 7 8 9 10 ------- time taken 0:00:00.011910 ------- Template Variables # Within the config.yaml file, template variables can be specified by using the notation ${{ VAR }}, the following variables are currently recognised:\nVariable Description DATE Date in the form %Y%m%d DATETIME Date and time in the form %Y-%m-%sT%H:%M:S DATETIME-%Y%H%M Date and time in custom format (where %Y%H%M can be any valid form) USER The current user as defined in the CLI REPO_DIR The FAIR repository root directory CONFIG_DIR The directory containing the config.yaml after template substitution SOURCE_CONFIG Path of the user defined config.yaml GIT_BRANCH Current branch of the git repository GIT_REMOTE The URI of the git repository specified during setup GIT_TAG The latest tag on git "
      }
      ,"3": {
        "url": "/docs/data_registry/installation/",
        "relUrl": "/docs/data_registry/installation/",
        "title": "Installation Instructions",
        "doc": "Installation Instructions",
        "section": "FAIR Data Registry",
        "content": " Local FAIR data registry # The documentation for the registry is available here, and is the same for the local and remote registry.\nInstallation # There are a few alternative ways to install a local FAIR data registry and we describe the different options below.\nDependencies # The registry relies on the graphviz package to produce the schema visualisation and the provenance report, so you will need to follow graphviz installation process for your system before initialising a local registry.\nWindows Dependencies # To install and run the local registry on windows the following dependencies are required:\nChocolatey Once Chocolatety is installed the following dependencies can be installed using chocolatey:\n- [Python 3](https://community.chocolatey.org/packages/python/3.9.7) - [Curl](https://community.chocolatey.org/packages/curl) - [Git](https://community.chocolatey.org/packages/git) Install local registry (Linux / MAC OS) # To initialise a local registry, run the following command from your terminal:\n/bin/bash -c \u0026#34;$(curl -fsSL https://data.fairdatapipeline.org/static/localregistry.sh)\u0026#34; This will install the registry and all the related files will be stored in ~/.fair.\nTo run the server, run the script:\n~/.fair/registry/scripts/start_fair_registry Then, navigate to http://localhost:8000 in your browser to check that the server is up and running. A token will be automatically generated in ~/.fair/registry/token.\nTo stop the server, run the script:\n~/.fair/registry/scripts/stop_fair_registry Install specific branch # If you need to install a specific branch from the registry repository, you can replace \u0026lt;branch_name\u0026gt; with the branch in question in the following command:\ncurl -fsSL https://data.fairdatapipeline.org/static/localregistry.sh | /bin/bash -s -- -b \u0026lt;branch_name\u0026gt; Install local registry (Windows) # To initialise a local registry on windows, run the following commands from command prompt\ncurl https://data.fairdatapipeline.org/static/localregistry.bat \u0026gt; localregistry.bat localregistry.bat This will install the registry and all the related files will be stored in C:\\Users\\\u0026lt;username\u0026gt;\\.fair\nTo run the server, run the C:\\Users\\\u0026lt;username\u0026gt;\\.fair\\registry\\scripts\\start_fair_registry_windows.bat script, this will spawn the server in a new window. Navigate to http://localhost:8000 in you browser to check the server is up and running. A token will be automatically generated in C:\\Users\\\u0026lt;username\u0026gt;\\.fair\\registry\\token.\nTo stop the server switch to the server window and press control + break or run: C:\\Users\\\u0026lt;username\u0026gt;\\.fair\\registry\\scripts\\stop_fair_registry_windows.bat.\nDefault Credentials # By default the local registry creates a superuser for the admin console with the username: admin and the password: admin\nUsing Vagrant in a local VM # An alternative to run the local registry without worrying about dependencies is to rely on Vagrant and a virtualisation engine such as VirtualBox.\nThe FAIR data registry codebase provides a Vagrantfile with the details on how to configure and provision a local virtual machine to run the data registry.\nSo, the steps to follow are:\nClone the repository:\ngit clone https://github.com/FAIRDataPipeline/data-registry.git Run Vagrant:\nvagrant up and your local FAIR data registry should be running at http://localhost:8000/.\nIf you then need to get into the VM for managing the local registry, you cn use the following commands:\nvagrant ssh sudo su cd /code/data-registry/ Then, to stop the data registry server you can use the following command:\nscripts/stop_fair_registry And to start it again, you can use:\nscripts/start_fair_registry_vagrant If you need to access the python/Django commands, still within the VM, you can activate the python virtual environment, add a Django environment variable and check the manage.py options as follows:\n. venv/bin/activate export DJANGO_SETTINGS_MODULE=drams.vagrant-settings python manage.py --help For example, you can use the following command to add some example data:\npython manage.py add_example_data Note that if you want to remove the VM, you can run the command:\nvagrant destroy "
      }
      ,"4": {
        "url": "/docs/quick_start/",
        "relUrl": "/docs/quick_start/",
        "title": "Quick Start",
        "doc": "Quick Start",
        "section": "Docs",
        "content": "The FAIR Data Pipeline needs three components to be installed on your computer in order to run. These are:\nCommand Line Interface (CLI) (Local) Data Registry A Modelling API (in whatever programming language you want to use) Prerequisites # The CLI needs to be installed using pip (the package installer for Python), which means you will need a Python distribution ( ) installed on your computer. You will also need to install Graphviz, if you would like to view provenance reports in the web interface.\nInstall the Command Line Interface (CLI) # Follow Instructions to install the CLI in the CLI GitHub repository. Install the (Local) Data Registry # Follow the instructions to install the Data Registry via the CLI. Select and install a Modelling API # Installation instructions for the Modelling APIs are in their respective GitHub repositories:\nC++ Java Julia Python R Try out an example # Example models using the FAIR Data Pipeline are provided in each of the supported languages. Instructions to install and run these examples can be found in their respective GitHub repositories:\nC++ Java Julia (The Julia example is installed along with the Julia Modelling API) Python R "
      }
      ,"5": {
        "url": "/docs/API/python/file_api/",
        "relUrl": "/docs/API/python/file_api/",
        "title": "File API",
        "doc": "File API",
        "section": "Python",
        "content": " File API # The file API manages file access, provenance, and metadata # The API is accessed as a \u0026ldquo;session\u0026rdquo;. All reads and writes are recorded and logged into a file when the session closes. Files are identified by their metadata, though the metadata is handled differently for reads (where the files are expected to exist) and writes (where they typically do not), described in more detail below.\nThe file API behaviour is entirely determined by a yaml configuration file (referred to here as a “config.yaml” file, and described below) provided at initialisation. This configuration file defines the “data directory” that the file API should interact with. That directory must contain a file called “metadata.yaml”, described below, that defines the metadata associated with the files in the data directory. The data directory and the metadata.yaml file can be automatically created by a download script which reads the config.yaml file and downloads appropriate data and metadata.\nWhen a model or script is run (in the “run”), any output files are written to the data directory, and an “access.yaml” file is created that enumerates exactly which files were read and written to during the run. The access.yaml file contains sufficient information to upload all of the data and metadata from the run to the data store and data registry respectively. This can be carried out automatically using an upload script if desired. Note that the access.yaml file may not be written until the connection to the API is closed (this is certainly true for the python implementation). When the file API is initialised a “run_id” is created to uniquely identify that invocation. It is constructed by forming the SHA1 hash of the configuration file content, plus the date time string.\nFor normal modelling runs, the only interaction with the File API happens through setting the config.yaml file (and running the download and upload scripts), but the rest of the information (formats of the metadata.yaml and access.yaml file, and the low-level File API calls themselves are provided here for completeness).\nconfig.yaml file format # The config file lets users specify metadata to be used during file lookup, and configure overall file API behaviour. A simple example:\ndata_directory: . access_log: access-{run_id}.yaml run_metadata: description: A test model data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: SCRC default_output_namespace: model_test submission_script: model.py remote_uri: ssh://boydorr.gla.ac.uk/srv/ftp/scrc/ remote_uri_override: ftp://boydorr.gla.ac.uk/scrc/ read: - where: data_product: human/commutes use: version: 1.0 - where: data_product: human/population use: filename: my-human-population.csv write: - where: data_product: human/outbreak-timeseries Component: use: namespace: simple_network_sim data_product: human/outbreak-timeseries data_directory specifies the file system root used for data access (default “.”). It may be relative; in which case it is relative to the directory containing the config file. The data directory must contain a metadata.yaml file.\naccess_log specifies the filename used to record the access log (default “access-{run_id}.yaml”). It may be relative; in which case it is relative to the directory containing the config file. It may contain the string {run_id}, which will be replaced with the run id. It may be set to the boolean value False to indicate that no access log should be written.\nrun_id specifies the run id to be used, otherwise a hash of the config contents and the date will be used.\nrun_metadata provides metadata for the run that will be passed through to the access log.\nThe where sections specify metadata subsets that are matched in the read and write processes. The metadata values may use glob syntax, in which case matching is done against the glob. The corresponding use sections contain metadata that is used to update the call metadata before the file access is attempted. A filename may be specified directly, in which case it will be used without any further lookup.\nAny other attributes will be ignored.\nmetadata.yaml file format # The metadata file contains metadata for all files in the file system “database”. A simple example:\n- data_product: human/commutes version: 1 extension: csv filename: human/commutes/1.csv verified_hash: 075abd810909918419cf7495c16f1afec6fa010c - data_product: human/compartment-transition version: 1 extension: csv filename: human/compartment-transition/1.csv verified_hash: 65662d0461471f36a06b32ca6d4003ca4493848f Each section defines the metadata for a single file, including its filename, relative to the directory containing the metadata.yaml file.\naccess.yaml format # The access file is generated whenever close() is called on the API. It records basic information about the run, and a log of file accesses. An example:\ndata_directory: . run_id: 84b87c5f60 open_timestamp: 2020-06-24 14:30:22.010927 close_timestamp: 2020-06-24 14:30:22.038766 config: ... run_metadata: git_repo: https://github.com/ScottishCovidResponse/simple_network_sim git_sha: 353697d0a04ef5d6d5a04ef9aef514cbd72a55fd ... io: - type: read timestamp: 2020-06-24 14:30:22.018370 call_metadata: data_product: human/mixing-matrix extension: csv access_metadata: data_product: human/mixing-matrix version: 1.0.0 extension: csv filename: human/mixing-matrix/1.csv verified_hash: 075abd810909918419cf7495c16f1afec6fa010c calculated_hash: 075abd810909918419cf7495c16f1afec6fa010c - type: write timestamp: 2020-06-24 14:30:22.038511 call_metadata: data_product: human/estimatec extension: csv access_metadata: data_product: human/estimatec extension: csv source: simple_network_sim filename: human/estimatec/84b87c5f60.csv calculated_hash: 91a6791ab4f6d3a4616066ffcae641ca3da79567 data_directory specifies the file system root used for data access, either as an absolute path, or relative to the config.yaml file used to generate the run.\nrun_id specifies the run id of the run.\nconfig reproduces the config.yaml used to generate this run verbatim.\nrun_metadata contains additional metadata about the file API execution taken from the config and possibly overridden using the set_run_metadata function.\nopen_timestamp and close_timestamp record time at which the file API was initialised, and the time at which close() was called.\nio points to a list of file access sections containing a common format:\ntype is either read or write.\ntimestamp is the timestamp of the access.\ncall_metadata contains the metadata provided to the open_for_read or open_for_write call.\naccess_metadata contains the metadata used to open the file. The process for obtaining this metadata is described in the open_for_read process and open_for_write process sections below.\nopen_for_read process # Search config for all read sections that are a subset of the given metadata and update the call metadata with the corresponding overrides. Search the metadata file for all metadata sections that are a superset of the updated metadata; if any results, use the metadata section with the highest version. Use the filename defined in the metadata; fail if there is no filename specified, or if the file is not found. Calculate the hash of the file and store it in the metadata. If hash verification is enabled, check that there is a verified hash in the metadata, and that it matches the calculated hash; fail otherwise. Record the read. Open the file for read and return the file handle. open_for_write process # Search config for all write sections that are a subset of the given metadata and update the call metadata with the corresponding overrides. If the metadata does not contain a filename, use the metadata and the run id to construct a standard filename, and add it to the metadata. Create all missing parent directories. If the file already exists, open the file for update, else open the file for write (and thus create it implicitly). Register a call-back to record the write on close and return the open file handle. The File API consists of five logical functions (in python, implementation details may vary):\nFunction Description init(configuration_filename) Initialise the API with a configuration file. open_for_read(metadata) Use metadata to open a file for reading. open_for_write(metadata) Use the metadata to open a file for writing. close() Write the access log to disk. May be called again. set_run_metadata(key, value) Associate a (key, value) metadata pair with the run. This is used by the Standard API to transmit the model uri and git_sha to the access.yaml. "
      }
      ,"6": {
        "url": "/docs/cli/",
        "relUrl": "/docs/cli/",
        "title": "Fair CLI",
        "doc": "Fair CLI",
        "section": "Docs",
        "content": " Data pipeline # The data pipeline is moving towards this new structure:\nThe API is accessed as a session, which corresponds in the registry to a code run. All reads and writes are logged directly to a local registry as the session progresses, as is the script which generates that i/o. Files are identified by their metadata, though the metadata is different for reads (where the files must exist) and writes (where they must not). The metadata is also different for files in core pipeline data formats, where a significant amounts of metadata are recorded, and other arbitrary files, where only a limited amount of data can be collected. Either type of file can be used for input or output, giving a total of four different interactions, two for input and two for output. These differences are described in more detail below.\nThe underlying data to which the API refers is determined by the interaction between a yaml configuration file (referred to here as a config.yaml file, and described below) provided at initialisation and the state of the remote registry at the time of processing of the config.yaml by the download synchronisation script (which is considered to contain the definitive version of all data at that time). The specific remote registry used is itself defined in the config.yaml file.\nThis interaction between the configuration file and the remote registry defines the “local filesystem data repository” that the local pipeline interacts with. The data directory can be automatically created by a download synchronisation script (currently found here) which reads the config.yaml file, queries the appropriate remote registry, downloads appropriate data, and populates the local registry with the relevant metadata for those data.\nWhen a model or script is run (as a session / “code run”), any output files are written to the data directory, and those outputs are logged in the local registry, which has itself been created (or updated) by the download synchronisation script. The local registry can be queried to determine whether the data generated is as intended, and if so it can then by synchronised back to the remote registry. This can be carried out automatically using an upload synchronisation script (currently here). When the session is initialised a “run id” is created to uniquely identify that code run. It is constructed by forming the SHA1 hash of the configuration file content, plus the date time string.\n"
      }
      ,"7": {
        "url": "/docs/cli/cli_functions/",
        "relUrl": "/docs/cli/cli_functions/",
        "title": "FAIR CLI functions",
        "doc": "FAIR CLI functions",
        "section": "Fair CLI",
        "content": " fair CLI functions # Note that this is a living document and the following is subject to change.\nA simple example of how the data pipline should run from the command line:\nfair pull config.yaml fair run config.yaml fair push config.yaml fair pull # download any data required by read: from the remote data store and record metadata in the data registry (whilst editing relevant entries, e.g. storage_root) pull meta data associated with all previous versions of these objects listed in write: from the remote data registry download any data listed in register: from the original source and record metadata in the data registry fair run # read (and validate) the config.yaml file generate a working config.yaml file (see Working example) globbing is used to interpret * as all matching objects as well as the original string returned, e.g. if real/data/1 version 0.0.1 and real/data/thing/1 version 0.0.1 already exist in the registry, the user-written config:\nwrite: - data_product: real/data/* description: general description for all data products use: namespace: someone version: ${{MINOR}} should return:\nwrite: - data_product: real/data/1 use: data_product: real/data/1 description: general description for all data products version: 0.1.0 namespace: someone - data_product: real/data/thing/1 use: data_product: real/data/thing/1 description: general description for all data products version: 0.1.0 namespace: someone - data_product: real/data/* use: data_product: real/data/* description: general description for all data products version: 0.0.1 namespace: someone specific version numbers and any variables in run_metadata:, register:, read:, and write: are replaced with true values, e.g.\n${{CONFIG_DIR}} is replaced by the directory within which the working config.yaml file resides release_date: ${{DATETIME}} is replaced by release_date: 2021-04-14T11:34:37 which is a valid form for the registry. version: 0.${{DATE}}.0 is replaced by version: 0.20210414.0 version: ${{PATCH}} should increment version by patch; and version: 0.${{DATETIME-%Y%m%d}}.0 or any variants thereof are replaced by an appropriately formatted string. if no version is given, then one should be written such that patch is incremented if the data product already exists, otherwise version should be set to 0.0.1.\nregister: is removed and external_objects are written to read: as data_products\npopulate public: field in run_metadata: section (default is true)\npopulate version: field in use: section of whether the user-written config contained the field or not\nlocal_repo: must always be given in the config.yaml file ensure the repo is clean get the hash of the latest commit and add to the working config.yaml file in run_metadata: latest_commit: if run_metadata: remote_repo: is false, then fair push should copy the repo to the file store if run_metadata: remote_repo: is absent or doesn\u0026rsquo;t contain a URL, then fair run should try to get the remote repo url from the local repo note that there are exceptions and the user may reference a script located outside of a repository save the working config.yaml file in the local data store, in \u0026lt;local_store\u0026gt;/coderun/\u0026lt;date\u0026gt;T\u0026lt;time\u0026gt;/config.yaml, e.g. datastore/coderun/20210625T165552/config.yaml save the submission script to the local data store in \u0026lt;local_store\u0026gt;/coderun/\u0026lt;date\u0026gt;T\u0026lt;time\u0026gt;/script.sh note that config.yaml should contain either script: that should be saved as the submission script, or script_path: that points to the file that should be saved as the submission script save the path to \u0026lt;local_store\u0026gt;/coderun/\u0026lt;date\u0026gt;T\u0026lt;time\u0026gt;/ in the global environment as $FDP_CONFIG_DIR so that it can be picked up by the script that is run after this has been completed execute the submission script fair push # push new files (generated from write: and register:) to the remote data store record metadata in the data registry (whilst editing relevant entries, e.g. storage_root) "
      }
      ,"8": {
        "url": "/docs/data_registry/",
        "relUrl": "/docs/data_registry/",
        "title": "FAIR Data Registry",
        "doc": "FAIR Data Registry",
        "section": "Docs",
        "content": " FAIR Data Registry # The FAIR data pipeline requires to maintain a data registry as we have the following objectives:\nWe need to identify and set up a system which will let everything be managed, version controlled, etc, etc. We will need to co-ordinate people to curate incoming data, generating version controlled data resources in formats required by the modellers. We will need to co-ordinate people to curate incoming data, generating version controlled data resources in formats required by the statisticians. We will need to co-ordinate people to curate papers and reports, so these are available for critical review. We will need to co-ordinate statisticians to analyse the data curated under point (3), producing version controlled statistical estimates of key variables relevant to the parameterisation of the models, to a prescribed standard. We will need to co-ordinate reviewers to scrape estimates of key variables relevant to the parameterisation of the models from the literature, to a prescribed standard. With the modellers, we will need to identify appropriate functional forms to map these version controlled statistical estimates to the parametric form required by the models, and incorporate these into the workflows. We need a transparent mechanism to either allow modellers to choose parameter values from a portfolio of version controlled values, or to integrate these into a single ensemble estimate (preferably taking account of the credibility and relevance of each source). "
      }
      ,"9": {
        "url": "/docs/data_registry/schema/",
        "relUrl": "/docs/data_registry/schema/",
        "title": "Schema Description",
        "doc": "Schema Description",
        "section": "FAIR Data Registry",
        "content": "The FAIR data registry schema has been designed to address the different requirements around the information needed on different research objects associated with an epidemiological pipeline.\nMain entities # The main entities represented in the schema are around the data and the software for epidemiological models:\nDataProduct: A dataset that is used by or is generated by a model. ExternalObject: An external data object, i.e. one that has comes from somewhere other than being generated as part of the modelling pipeline. CodeRun: A code run along with its associated, code repo, configuration, input and outputs. CodeRepoRelease: Information marking that an Object is an official release of a model code. Object: Core traceability object used to represent any FAIR data object such DataProduct, CodeRepoRelease, CodeRun. Data representation # For the data representation, the schema also supports categorising datasets by using:\nNamespace: A namespace is a way to group DataProducts following some specific categorisation criteria, such as what is the organisation producing the data product. Models representation # FAIR research objects # The schema also represents FAIR objects, which are associated with DataProducts, CodeRuns and so on.\nThe Object is associated with:\nLicence: Licence that can be associated with an Object in case the code or data source has a specific licence that needs to be recorded. StorageLocation: The location of an item relative to a StorageRoot. StorageRoot: The root location of a storage cache where model files are stored. QualityControlled: Marks that the associated Object has been quality controlled. Keyword: Keywords that can be associated with an Object usually for use with ExternalObjects to record paper keywords, and so on. The object is also associated with entities related to users and authors:\nAuthor: Authors that can be associated with an Object usually for use with ExternalObjects to record paper authors, etc. UserAuthor: A combination of an Author associated with a particular user. Internal provenance # Finally, the schema also includes some entities that are used internally:\nBaseModel: Base model for all objects in the database. Used to defined common fields and functionality to keep internal provenance information such as when it was last updated and by whom. Schema Diagram # The whole schema diagram can be accessed on the remote data registry. We are also including the image here, which you can open in a new tab to see it full scale:\n"
      }
      ,"10": {
        "url": "/docs/data_registry/prov_report/",
        "relUrl": "/docs/data_registry/prov_report/",
        "title": "Provenance Report",
        "doc": "Provenance Report",
        "section": "FAIR Data Registry",
        "content": " Provenance Report # In order to address the use case around being able to track the evidence to understand the reported results, the data registry has the capability to produce provenance reports for each of the data products.\nProvenance is the documented history of processes in a digital object\u0026rsquo;s lifecycle.\nThe provenance reports generated by the registry are based around the concepts of activities, agents and entities. For more information about these concepts see the PROV Ontology or PROV-O.\nProvenance reports are only available for DataProducts and can be accessed via the RESTful API for example:\nhttps://data.fairdatapipeline.org/api/prov-report/3/ Query parameters # format\napi: a html representation of the report with media type of text/html json: a json representation of the report with media type of application/json json-ld: a json-ld representation of the report with media type of application/ld+json jpg: an image representing the provenance with media type of image/jpeg svg: an interactive image representing the provenance with media type of image/svg+xml xml: an XML representation of the report with media type of text/xml provn: a PROV-N representation of the report with media type of text/provenance-notation aspect_ratio\n\u0026lt;float\u0026gt;: a float used to define the ratio for the JPEG and SVG images. The default is 0.71, which is equivalent to A4 landscape. attributes\nTrue (default): show the attributes associated with an object on the image False: hide the attributes associated with an object on the image dpi\n\u0026lt;float\u0026gt;: a float used to define the dpi for the JPEG and SVG images depth\n\u0026lt;integer\u0026gt;: an integer used to determine how many levels of code runs to include, the default is 1 Prefixes # All activities, agents and entities have a URI. Prefixes are used to represent the base component of these URIs. Two different prefixes are used, reg and lreg, where reg is used as the prefix for the central registry and lreg is used to refer to a local registry. Hence dependent on the location of the object you may see:\nreg:api/data_product/1 or\nlreg:api/data_product/1 Using provn as an example you may see a section similar to:\nprefix lreg \u0026lt;http://192.168.20.10:8000/\u0026gt; prefix fair \u0026lt;https://data.fairdatapipeline.org/vocab/#\u0026gt; prefix dcat \u0026lt;http://www.w3.org/ns/dcat#\u0026gt; prefix dcmitype \u0026lt;http://purl.org/dc/dcmitype/\u0026gt; prefix dcterms \u0026lt;http://purl.org/dc/terms/\u0026gt; prefix foaf \u0026lt;http://xmlns.com/foaf/spec/#\u0026gt; Examples # Basic Example # For a simple case the report will contain two entities, a DataProduct and an ExternalObject, where the DataProduct is a specializationOf an ExternalObject.\nAn example of a basic provenance diagram # And this is an example of the XML that is produced:\n\u0026lt;prov:document xmlns:lreg=\u0026#34;http://192.168.20.10:8000/\u0026#34; xmlns:fair=\u0026#34;https://data.fairdatapipeline.org/vocab/#\u0026#34; xmlns:dcat=\u0026#34;http://www.w3.org/ns/dcat#\u0026#34; xmlns:dcmitype=\u0026#34;http://purl.org/dc/dcmitype/\u0026#34; xmlns:dcterms=\u0026#34;http://purl.org/dc/terms/\u0026#34; xmlns:foaf=\u0026#34;http://xmlns.com/foaf/spec/#\u0026#34; xmlns:prov=\u0026#34;http://www.w3.org/ns/prov#\u0026#34; xmlns:xsd=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;\u0026gt; \u0026lt;prov:entity prov:id=\u0026#34;lreg:api/data_product/1\u0026#34;\u0026gt; \u0026lt;prov:type xsi:type=\u0026#34;xsd:QName\u0026#34;\u0026gt;dcat:Dataset\u0026lt;/prov:type\u0026gt; \u0026lt;dcat:hasVersion\u0026gt;0.20210915.0\u0026lt;/dcat:hasVersion\u0026gt; \u0026lt;dcterms:description\u0026gt;Static parameters of the model\u0026lt;/dcterms:description\u0026gt; \u0026lt;dcterms:format\u0026gt;Comma-Separated Values File\u0026lt;/dcterms:format\u0026gt; \u0026lt;dcterms:modified xsi:type=\u0026#34;xsd:dateTime\u0026#34;\u0026gt;2021-09-15T14:16:42.899768+00:00\u0026lt;/dcterms:modified\u0026gt; \u0026lt;dcterms:title\u0026gt; disease/sars_cov2/SEIRS_model/parameters/static_params \u0026lt;/dcterms:title\u0026gt; \u0026lt;fair:namespace\u0026gt;PSU\u0026lt;/fair:namespace\u0026gt; \u0026lt;prov:atLocation\u0026gt; file:///var/folders/0f/fj5r_1ws15x4jzgnm27h_y6h0000gr/T/tmpukqzlyig/data_store//PSU/disease/sars_cov2/SEIRS_model/parameters/static_params/0.20210915.0.csv \u0026lt;/prov:atLocation\u0026gt; \u0026lt;/prov:entity\u0026gt; \u0026lt;prov:person prov:id=\u0026#34;lreg:api/author/1\u0026#34;\u0026gt; \u0026lt;foaf:name\u0026gt;Interface Test\u0026lt;/foaf:name\u0026gt; \u0026lt;/prov:person\u0026gt; \u0026lt;prov:wasAttributedTo\u0026gt; \u0026lt;prov:entity prov:ref=\u0026#34;lreg:api/data_product/1\u0026#34;/\u0026gt; \u0026lt;prov:agent prov:ref=\u0026#34;lreg:api/author/1\u0026#34;/\u0026gt; \u0026lt;prov:role xsi:type=\u0026#34;xsd:QName\u0026#34;\u0026gt;dcterms:creator\u0026lt;/prov:role\u0026gt; \u0026lt;/prov:wasAttributedTo\u0026gt; \u0026lt;prov:entity prov:id=\u0026#34;lreg:api/external_object/1\u0026#34;\u0026gt; \u0026lt;prov:type xsi:type=\u0026#34;xsd:QName\u0026#34;\u0026gt;dcat:Dataset\u0026lt;/prov:type\u0026gt; \u0026lt;dcat:hasVersion\u0026gt;0.20210915.0\u0026lt;/dcat:hasVersion\u0026gt; \u0026lt;dcterms:issued xsi:type=\u0026#34;xsd:dateTime\u0026#34;\u0026gt;2021-09-15T15:16:42+00:00\u0026lt;/dcterms:issued\u0026gt; \u0026lt;dcterms:title\u0026gt;Static parameters of the model\u0026lt;/dcterms:title\u0026gt; \u0026lt;fair:alternate_identifier\u0026gt; SEIRS model parameters - Static parameters of the model \u0026lt;/fair:alternate_identifier\u0026gt; \u0026lt;fair:alternate_identifier_type\u0026gt;SEIRS_model_params\u0026lt;/fair:alternate_identifier_type\u0026gt; \u0026lt;/prov:entity\u0026gt; \u0026lt;prov:specializationOf\u0026gt; \u0026lt;prov:specificEntity prov:ref=\u0026#34;lreg:api/external_object/1\u0026#34;/\u0026gt; \u0026lt;prov:generalEntity prov:ref=\u0026#34;lreg:api/data_product/1\u0026#34;/\u0026gt; \u0026lt;/prov:specializationOf\u0026gt; \u0026lt;/prov:document\u0026gt; A Data Product Generated from a Code Run # In a complete example a DataProduct entity would have a relationship of wasGeneratedBy with a CodeRun activity, it would have a relationship of wasAttributedTo with an Author agent and it would have a relationship of wasDerivedFrom one or more DataProducts entities.\nIn turn the CodeRun would have a relationship of wasStartedBy with an Author, it would have used a model_cofiguration, submission_script, CodeRepoRelease and one or more DataProducts\nAn example of a provenance diagram # "
      }
      ,"11": {
        "url": "/docs/API/python/terminology/",
        "relUrl": "/docs/API/python/terminology/",
        "title": "Terminology",
        "doc": "Terminology",
        "section": "Python",
        "content": " Terminology used in this document # datum A specific value, encoded in a particular way, that travels through the data pipeline. config.yaml A file (potentially with a different name) used by the data pipeline to allow users to override default pipeline behaviour. See the File API specification for more details. metadata.yaml A file used by the data pipeline to describe available data files, listing their associated metadata. See the File API specification for more details. access.yaml A file (potentially with a different name) generated by the data pipeline API to record file access. See the File API specification for more details. Metadata # data_product Identifies which kind of quantity a datum represents (e.g. “human/mixing-matrix”). Path-formatted to permit structure in the filename scheme (defined below). The desired data_product is typically specified in model code, and it is a core part of the data identifiers used in config.yaml, metadata.yaml, and access.yaml. version A semver identifying a version of a data_product (the file API will select the most recent version if this is not specified). component Identifies a part of a data_product. filename Specifies the path to a file, typically relative to the data root. Only required on read, and typically inferred from metadata.yaml. extension Specifies the extension of a file. Required on write to generate a standard filename. Typically provided by a datatype API. run_id Specifies a unique identifier for a model run. Required on write to generate a standard filename, typically generated by the file API. verified_hash Specifies a \u0026ldquo;verified good” SHA1 hash for a file. Used by the file API to verify file contents. Typically defined in metadata.yaml. calculated_hash Specifies the SHA1 hash computed by the file API for a file. Typically only defined in access.yaml. max_warning Specifies the maximum known warning level for a particular datum. Could be used by the file API to filter “bad” data (currently not supported). Typically defined in metadata.yaml. Filenames # {data root}/{data_product}\u0026hellip;/{run_id}.{extension}\ne.g. {data root}/human/mixing-matrix/12345.h5\n"
      }
      ,"12": {
        "url": "/docs/data_registry/ro_crate/",
        "relUrl": "/docs/data_registry/ro_crate/",
        "title": "RO Crate Objects",
        "doc": "RO Crate Objects",
        "section": "FAIR Data Registry",
        "content": " Research Object Crate (RO-Crate) # "
      }
      ,"13": {
        "url": "/docs/sync_notes/",
        "relUrl": "/docs/sync_notes/",
        "title": "Notes on synchronisation",
        "doc": "Notes on synchronisation",
        "section": "Docs",
        "content": " Notes # These notes are just extracted from Zulip, so are more stream of consciousness than specifications, but they attempt to describe what a command line tool might do that carried out syncing between the local and remote registries, and what else we might want it to do. Given some of the functionality described here is already provided by the download and upload scripts in the data_pipeline_api repo, it probably makes sense for this to be written in python.\nThere is an alternative to this command line solution, which would be for the Django interface to the local registry to provide the same (or similar) functionality. This would allow the commands to be access through the web interface, or through the REST API and (optionally) also through the native language interfaces. This would not preclude a command line option as well, because it would simply be a wrapper around the REST API.\nDistinction between modelling API and backend # Individual languages communicate with the local registry and the local data store, but communicate no further afield. They need to be able to parse the config.yaml file to understand the relationship between the api requests they receive and the requests they need to make of the registry, however.\nAll of the syncing:\nbetween the remote registry and the local registry to populate the local data store by downloading data from remote data stores to create remote storage locations for local data that is being pushed to the remote registry is handled by an as-yet-unwritten command line tool, which (for the sake of convenience) I have been calling fdp. This will act a little bit like git. I\u0026rsquo;m currently imagining it will look like:\nfdp pull config.yaml will pull all of the registry entries relevant to this config file into the local registry, and download any files that are referred to in read: or register: blocks to the local data store. fdp run config.yaml will run the submission script referred to in the config.yaml fdp commit \u0026lt;some-token\u0026gt; | \u0026lt;path/to/file\u0026gt; will mark a code run or a specific file (and all of its provenance) to be synced back to the remote registry (once we\u0026rsquo;ve worked out how to refer to code runs in an intuitive way?) fdp push will sync back any committed changes to the remote registry and upload any associated data to the remote data store (but we still haven\u0026rsquo;t determined how to specify what to store where) Other commands that might be useful:\nfdp status report on overall sync status of registry fdp status \u0026lt;some-token\u0026gt; | \u0026lt;path/to/file\u0026gt; report on sync status of a specific thing (e.g. a code run) or a specific file fdp update pull the most recent registry entries (and associated data) for anything in the current registry fdp gc [--files [--all]] maybe to remove anything from the registry that is not in the provenance history of an unsynced file? Add --files to also delete the associated files from the local data store unless they have restricted accessibility. Add --all to also delete the files from the local data store even if they have restricted accessibility. fdp delete [--force] \u0026lt;some-token\u0026gt; or fdp delete [--force] \u0026lt;path/to/file\u0026gt; delete a specific thing (e.g. a code run) or a specific file from the registry and local data store, adding --force if it is unsynced. fdp info \u0026lt;some-token\u0026gt; | \u0026lt;path/to/file\u0026gt; - return metadata or registry information on a specific registry entry or file The fdp code will handle syncing between two registries using two REST APIs, one at each end, which (I think?) is a significantly harder problem, and so is probably best only solved once though!\nWe can\u0026rsquo;t see a way of easily allowing the functionality we need without having a local registry. I think the idea is that the local registry is going to be as standalone as possible. At the moment it\u0026rsquo;s a single command to install it, and a second to start it up, a third to stop it. Ideally fdp might be able to check if it\u0026rsquo;s running and start it up and stop it on its own I guess, so the only step would be to install it.\nIn principle the \u0026ldquo;local\u0026rdquo; registry could be on a separate computer and exist for multiple users (say inside a safe haven or on HPC), but it would have to have access to the same filesystem as the machine you\u0026rsquo;re running on otherwise we get back into issues of remote data access again.\nSpecialising config.yaml files # One thing we haven\u0026rsquo;t quite resolved is the distinction between the state of the world when we do fdp pull config.yaml, and the state of the world when we do fdp run config.yaml. A few things may have happened:\nthe local registry may have been garbage collected using fdp gc, and now: stuff is missing that is required by the code run the remote registry may have been updated and: 2. we haven\u0026rsquo;t checked, so the local registry is out of date 3. we have run another fdp pull other-config.yaml and so some, but possibly not all, of the local registry entries may have been updated the remote registry may be updated during a run (possibly by calling fdp pull other-config.yaml for a different run), and now: 4. queries made later in a run are inconsistent with queries made earlier in a run. Note this has to happen because we want to be able to write then read something just created during a run - this is one of the requirements that necessitates having a local registry The first two of these are probably easy to handle (crash and ignore, respectively!), but the last two are more tricky. Those may (will?) have issues for the reproducibility of the code run, because there may be no way of being able to reproduce the state of the registry at the point at which it was run. I wonder whether fdp run config.yaml should actually generate a fully qualified config.yaml file specifically for that run, that lists the exact version and data product, etc. with no globbing or text replacement left for the API code to worry about?\nAlternatively this could happen when we call fdp pull config.yaml to generate config-2353563.yaml, and fdp run config-2353563.yaml could be executed on the fully qualified version of the file\u0026hellip;\nconfig-2353563.yaml was just the name for a file generated by fdp from config.yaml which removes all calculations and lookups, so that human/disease/SARS-CoV-2/* becomes a whole series of entries, one for each data product, which specifies namespace, data product and version so you know exactly what the right version was at the point you ran fdp.\nIf there\u0026rsquo;s a working version of the config.yaml that will reproducibly generate the right aliases no matter what you subsequently do to the registry, then it makes things much simpler (and because it is what the API code is actually using, we know that it will generate the right aliases).\nfdp pull config.yaml uses the original. Maybe fdp run config.yaml creates the new one and starts up the job, or maybe you have a specific fdp generate config.yaml config-working.yaml to generate the working one? The modelling code should never require an internet connection though.\nOn a related note to above, the specialised config-working.yaml file should have all of the run_metadata defaults explicitly stated so that changing your config won\u0026rsquo;t change the run\u0026hellip;\nGoing back to the lack of internet connectivity though, generating the working yaml file doesn\u0026rsquo;t require an internet connection though. It\u0026rsquo;s just saying \u0026ldquo;given the current state of the local registry, what does this config.yaml file really mean in practice if I use it?\u0026rdquo;.\nLocal data storage # How about this? You setup a new local registry, go to /Users/johnsmith/datastore and run fdp config \u0026quot;johnsmith\u0026quot; to set this directory as your data store. This will generate an API token for the local registry and add an entry to .scrc/.users noting the fact that the johnsmith username / namespace is associated with /Users/johnsmith/datastore\u0026hellip; or do something fancier to specify the local registry url, or even set passwords? Your config.yaml now contains a username / namespace rather than a list of defaults associated with your account.\nIf someone wants to share your datastore, that\u0026rsquo;s fine, they run fdp config \u0026quot;otherperson\u0026quot; from the same working directory and their details are added to .scrc/.users.\nfdp config might also generate a specific login node, recorded in .users?\n"
      }
      ,"14": {
        "url": "/docs/API/",
        "relUrl": "/docs/API/",
        "title": "Modelling APIs",
        "doc": "Modelling APIs",
        "section": "Docs",
        "content": " API # The API is defined more in terms of file formats than it is in terms of data types. There are two file formats that are native to the data pipeline, and files in these formats are referred to as data products: TOML files, and HDF5 files. TOML files store “small” parameter data, representing individual parameters. HDF5 files are used to store structured data, encoded either as “arrays” or “tables”. Both formats are described in more detail below, alongside API functions used to interact with them. Data in any other file format are treated as binary blobs, and are referred to as external objects.\nDifferent metadata is stored about each \u0026ndash; data products record information about their internal structure and naming of their components, whereas external objects record information about their provenance (since data products are internal to the pipeline, provenance is recorded separately). A single object can be both an external object and a data product, and thus have both sets of metadata recorded.\nInitialisation # The API must be initialised with the model URI and git sha, which should then be set as run-level metadata.\nAdditional metadata on write # The write functions all accept description and issues arguments.\nTOML (parameter) files # A parameter file contains representations of one or more parameters, each a single number, possibly with some associated uncertainty. Parameters may by represented as point-estimates, parametric distributions, and sample data.\nFile format # Parameters are stored in toml-formatted files, with the extension “toml”, containing sections corresponding to different components. The following is an example of the internal encoding, defining three components: \u0026ldquo;my-point-estimate\u0026rdquo;, \u0026ldquo;my-distribution\u0026rdquo;, and \u0026ldquo;my-samples\u0026rdquo;:\n[my-point-estimate] type = \u0026#34;point-estimate\u0026#34; value = 0.1 [my-distribution] type = \u0026#34;distribution\u0026#34; distribution = \u0026#34;gamma\u0026#34; shape = 1 scale = 2 [my-samples] type = \u0026#34;samples\u0026#34; samples = [1.0, 2.0, 3.0, 4.0, 5.0] Point estimates are used when our knowledge of the parameter is only sufficient for a single value, with no notion of uncertainty. A point estimate component must have type = \u0026ldquo;point-estimate\u0026rdquo; and a value that is either a float or an integer.\nDistributions are used when our knowledge of a parameter can be represented by a parametric distribution. A distribution component must have type = \u0026ldquo;distribution\u0026rdquo;, a distribution set to a string name of the distribution, and other parameters determined by the distribution. The set of distributions required to be supported is currently undefined.\nSamples are used when our knowledge of a parameter is represented by samples, from either empirical measurements, or a posterior distribution. A samples component must have type = \u0026ldquo;samples\u0026rdquo; and a value that is a list of floats and integers.\nDistributions # The supported distributions,each with a link to information about their parameterisation, and their standardised parameter names are as follows:\nDistribution Standardised parameter names categorical (non-standard) bins (string array), weights (float array) gamma k (float), theta (float) normal mu (float), sigma (float) uniform a (float), b (float) poisson lambda (float) exponential lambda (float) beta alpha (float), beta (float) binomial n (int), p (float) multinomial n (int), p (float array) API functions # read_estimate(data_product, component) -\u0026gt; float or integer\nIf the component is represented as a point estimate, return that value.\nIf the component is represented as a distribution, return the distribution mean.\nIf the component is represented as samples, return the sample mean.\nread_distribution(data_product, component) -\u0026gt; distribution object\nIf the component is represented as a point estimate, fail.\nIf the component is represented as a distribution, return an object representing that distribution.\nIf the component is represented as samples, failreturn an empirical distribution.\nread_samples(data_product, component) -\u0026gt; list of floats or integers\nIf the component is represented as a point estimate, fail.\nIf the component is represented as a distribution, fail.\nIf the component is represented as samples, return the samples.\nwrite_estimate(data_product, component, estimate, description, issues)\nwrite_distribution(data_product, component, distribution object, description, issues)\nwrite_samples(data_product, component, samples, description, issues)\nHDF5 files # Note that the following is subject to change. For example, we may want to add all of the metadata as attributes.\nAn HDF5 file can be either a table or an array. A table is always 2-dimentional and might typically be used when each column contains different classes of data (e.g. integers and strings). Conversely, all elements in an array should be the same class, though the array itself might be 1-dimensional, 2-dimensional, or more (e.g. a 3-dimensional array comprising population counts, with rows as area, columns as age, and a third dimension representing gender).\nYou should create a single HDF5 file for a single dataset. Unless you have a dataset that really should have been generated as multiple datasets in the first place (e.g. testing data mixed with carehome data), in which case use your own judgement.\nHDF5 files contain structured data, encoded as either an “array”, or a “table”, both of which are described in more detail below.\nFile format # HDF5 files are stored with the extension “h5”. Internally, each component is stored in a different (possibly nested) group, where the full path defines the component name (e.g. “path/to/component”). Inside the group for each component is either a value named “array”, or a value named “table”. It is an error for there to be both.\narray format # {component}/array An n-dimensional array of numerical data {component}/Dimension_{i}_title The string name of dimension \\(i\\) {component}/Dimension_{i}_names String labels for dimension \\(i\\) {component}/Dimension_{i}_values Values for dimension \\(i\\) {component}/Dimension_{i}_units Units for dimension \\(i\\) {component}/units Units for the data in array table format # {component}/table A dataframe {component}/row_names String labels for the row axis {component}/column_units Units for the columns API functions # read_array(data_product, component) -\u0026gt; array\nIf the component does not terminate in an array-formatted value, raise an error.\nReturn an array, currently with no structural information.\nread_table(data_product, component) -\u0026gt; dataframe\nIf the component does not terminate in a table-formatted value, raise an error.\nReturn a dataframe, with labelled columns.\nwrite_array(data_product, component, array, description, issues)\nIf the array argument is not array-formatted, raise an error.\nwrite_table(data_product, component, table, description, issues)\nIf the table argument is not table-formatted, raise an error.\n"
      }
      ,"15": {
        "url": "/docs/jargon_buster/",
        "relUrl": "/docs/jargon_buster/",
        "title": "Jargon buster",
        "doc": "Jargon buster",
        "section": "Docs",
        "content": " Jargon buster # C # config.yaml a file that specifies the metadata used during file lookup (for read, write, and register), used to configure the overall API behaviour. D # Data product objects that are accessable with the standard API read_xxx(), write_xxx() calls (i.e. toml or hdf5 files) Data registry a database for recording metadata about data and model outputs E # External object an object that is not in an internal pipeline format (i.e. not toml or hdf5 files) F # finalise() calculates the hash of the external objects / data products and renames files, check files exist, etc. I # initialise() reads in the config.yaml file, generates a new entry in code_run and returns id, creates new submission_script (and if necessary creates a new code_repo) entry in local registry L # link_read() connects code_run to external_object as input and returns path location (see also link_write()) link_write() connects code_run to data_product as output and returns path location (see also link_read()) R # read: a field in config.yaml used to specify the reading of an object (a data product or an external object) that belongs to the user (see also write: and register:) register: a field in config.yaml used to specify the registration of an object (usually an external object) that exists elsewhere (see also write: and read:) add_to_register() registers the object(s) listed in register: (in the config.yaml file) W # write: a field in config.yaml used to specify the writing of an object (a data product or an external object) that belongs to the user (see also register: and read:) "
      }
      ,"16": {
        "url": "/docs/repository_status/",
        "relUrl": "/docs/repository_status/",
        "title": "Repository Status",
        "doc": "Repository Status",
        "section": "Docs",
        "content": "Work in progress. All blank cells are TBDs. Idea is not to duplicate the software checklist or other information held in repositories, but to summarise it in one place.\nCI Dashboard # FAIRDataPipeline_FDP-Cpp-API\nRepository CI Status Coverage Default branch Static Analysis FAIR-CLI develop data-registry main unknown cppDataPipeline main DataPipeline.jl main unknown javaDataPipeline main Code style checked on commit (not automated) pyDataPipeline dev unknown rDataPipeline main unknown cppSimpleModel NA main NA Julia example? javaSimpleModel NA master NA pySimpleModel NA main NA rSimpleModel NA main NA Note: static analyusis may nto be available or appropriate for all languages.\nDocumentation # Repository Remarks FAIR-CLI Docs in readme and in docs folder, doscstrings not currently used to build docs. data-registry User docs on Hugo site. Dev docs in docs folder of repo. cppDataPipeline Automated docs built to website https://www.fairdatapipeline.org/cppDataPipeline/ DataPipeline.jl Automated docs built to website https://www.fairdatapipeline.org/DataPipeline.jl/stable/, also https://www.fairdatapipeline.org/docs/API/julia/ javaDataPipeline Automated docs built to website https://www.fairdatapipeline.org/javaDataPipeline/, also user docs https://www.fairdatapipeline.org/docs/API/Java/ pyDataPipeline Automated docs built to website https://www.fairdatapipeline.org/pyDataPipeline/, also https://www.fairdatapipeline.org/docs/API/python/ rDataPipeline Automated docs built to website https://www.fairdatapipeline.org/rDataPipeline/, also https://www.fairdatapipeline.org/docs/API/R/ cppSimpleModel https://github.com/FAIRDataPipeline/cppSimpleModel#readme Julia example? javaSimpleModel https://github.com/FAIRDataPipeline/javaSimpleModel#readme pySimpleModel https://github.com/FAIRDataPipeline/pySimpleModel#readme rSimpleModel https://github.com/FAIRDataPipeline/rSimpleModel#readme Status # Repository Type Maintained Developer Docs User Docs Contributing Guide Tests Static Analysis chocolatey-packages cppDataPipeline Modelling API Yes cppSimpleModel Example Yes data-registry Registry Yes DataPipeline.jl Modelling API Yes FAIR-CLI CLI Yes fair-setup-action fairdatapipeline.github.io FDP_hugo Website Yes FDP_validation homebrew-fairdatapipeline javaDataPipeline Modelling API Yes javaSimpleModel Example Yes pyDataPipeline Modelling API Yes https://www.fairdatapipeline.org/pyDataPipeline/ pySimpleModel Example Yes rDataPipeline Modelling API Yes rSimpleModel Example Yes "
      }
      ,"17": {
        "url": "/docs/use_cases/",
        "relUrl": "/docs/use_cases/",
        "title": "Uses cases",
        "doc": "Uses cases",
        "section": "Docs",
        "content": " Use Case 1. (Pre-process and) upload a new dataset using script/config file # Actor: A Data Curator wants to upload a dataset to the registry \u0026ndash; this will likely involve a pre-processing step to convert a \u0026lsquo;raw dataset\u0026rsquo; into a suitable \u0026lsquo;processed dataset\u0026rsquo; that can be used by a model. Has knowledge of system and required config files to produce an upload script\nPreconditions: Actor has an account (if required) to access data registry and a local copy of the dataset they want to upload (QUESTION: what formats are acceptable?)\nBasic Flow:\nActor generates metadata file for dataset\nActor generates a config file that gives name and location for data, metadata, desired name on registry\nSubsteps needed?\nDo we want an Actor to confirm here that they have permission to upload/share the data?\nActor logs into system, system authenticates (Question: or should this be part of the config file?)\n(optionally) Actor runs processing script to turn a dataset into the data format required\nActor runs upload script, system responds and ingests data and metadata\nIncluded in system response: system compares to existing data (possible exception case: user tries to upload dataset already in the registry) System generates report summarising upload, including date and hash, etc\nPostconditions:\nDatasets are now in registry with metadata - both the raw dataset and the processed dataset\nPre-processing script stored as code run associated with the dataset\nA response code from the API confirms that the upload has completed successfully\nUse Case 2. Uploading a new dataset via a GUI # Benefit: This would enable people who don\u0026rsquo;t know how to write scripts to casually interact with the system\nActor: A person who has found a dataset that they want to upload to the registry.\nPreconditions: User has an account (if required) to access data registry and a local copy of the dataset they want to upload (QUESTION: what formats are acceptable?)\nBasic Flow:\nActor logs into system, system authenticates\nActor selects web interface for adding a dataset to the registry, system loads page\nActor drags file to upload, system uploads and saves\nSystem prompts Actor to confirm they have permission to upload/share these data\nSystem prompts user for metadata, with form or upload option\nActor either uploads metadata file (Question: in what format?), or enters information in form\nSystem records metadata with dataset, creates data product and adds to registry\nSystem generates report summarising upload, including date and hash, etc\nPostconditions:\nDataset is now in registry with metadata\nActor has a report describing the upload of the data including a hash and date, and knows how and where to find those data\nExceptions:\nData is already uploaded? Do we allow reuploads? Questions\nWhere / how? Could happen in Django... already thought about doing this in a shiny app, should it generate a script that can be uploaded (and run via use case 1)?\nSupported formats for data and meta-data (eg supported data structures vs blobs)\nAuthorisation to write to certain namespaces?\nReproducing registry from scripts \u0026ndash; how to maintain if GUI route supported?\nUse Case 3. QuickStart Exploratory Dataset Use/Analysis # Benefit: Modeller able to make efficient and accurate assessment of usefulness of dataset for their purposes\nActor: A modelling or data scientist who wants to investigate and visualise a dataset programmatically in the pipeline to determine if it is suitable for their application.\nPreconditions:\nDataset actor is interested in is available via data pipeline\nActor has some technical proficiency in at least one language used by the pipeline, an interest in data, ability to visualise data within their language of choice\nBasic flow:\nActor searches for a data resource, system\u0026rsquo;s search ability provides a link to the data and a pointer to a description. Actor decides to investigate use of data resource further\nActor installs pipeline in their language of choice\nActor accesses data via pipeline from their local machine, system provides data into a data structure usable in actor\u0026rsquo;s preferred language\nWe\u0026rsquo;ll need sub-steps for this one, these might include:\nActor defines data they want (YAML file according to some spec)\nActor defines config according to some spec\nSystem recognises YAML/config files and provides data flow along with logs of access\nLocally-installed pipeline generates log files on actor\u0026rsquo;s machine\nActor loads data locally, performs plotting and exploration tasks\nPostconditions:\nLog files on actor\u0026rsquo;s local machine recording data access and use via pipeline\nActor has plots and/or exploratory description of dataset allowing them to judge suitability\nLog files on registry of access by actor if desired?\nQuestions:\nAuthentication in this workflow? Not for registry but maybe for individual datasets. How to show access restrictions to users in a convenient way?\nThinking needed about how to make this use case convenient for good 1^st^ impression\nLocal log files vs live local registry?\nUse Case 4. Model run # Benefit: Modeller is able to easily analyse dataset(s), generate output and upload to registry\nSimilar to use case 3, but end to end run, including outputs generated and upload of results to registry and data stores\nActor: A modelling or data scientist who wants to programmatically run an analysis and add the results to the pipeline\nPreconditions:\nDataset(s) actor is interested in is available via data pipeline\nActor has some technical proficiency in at least one language used by the pipeline, an interest in data, ability to visualise data within their language of choice\nBasic flow:\nActor searches for data resource(s), system\u0026rsquo;s search ability provides a link to the data and a pointer to a description. Actor decides to conduct analysis using data resources\nActor installs pipeline in their language of choice\nActor accesses data via pipeline from their local machine, system provides data into a data structure usable in actor\u0026rsquo;s preferred language\nWe\u0026rsquo;ll need sub-steps for this one, these might include:\nActor defines data they want (YAML file according to some spec)\nActor defines config according to some spec\nSystem recognises YAML/config files and provides data flow along with logs of access\nLocally-installed pipeline generates log files on actor\u0026rsquo;s machine\nActor loads data locally, performs analysis, generates outputs, all of which actions are traced using the pipeline API\nPostconditions:\nLog file use on actor\u0026rsquo;s local machine recording data access and use and outputs generated via pipeline\nActor has analytical results to investigate\nif model run was successful then outputs are uploaded to data stores and logs of access and creation of files by actor are stored on registry\nQuestions:\nAuthentication in this workflow? Not for registry but maybe for individual datasets. How to show access restrictions to users in a convenient way?\nLocal log files vs live local registry?\nUse Case 5. Parameter inference coupled with model simulation # This requires repeated application of two component use cases shown below:\nUse case: draw multiple data from data pipeline/registry for use in analysis (data for inference)\nUse case: Run model/analysis that generates multiple reps/samples from a potentially high dimensional distribution and then to add these to the data pipeline/registry tracking data dependencies (parameter inference)\nUse case: draw multiple data from data pipeline/registry for use in analysis (data to run model including inferred parameters)\nUse case: Run model/analysis that generates multiple reps/samples from a potentially high dimensional distribution and then to add these to the data pipeline/registry tracking data dependencies (parameter inference)\nActor: Researcher undertaking significant inference or analysis exercise couple with model simulation where repeated sampling and recording is required\nBenefit: really useful as a way of archiving derivatives of complex queries, potentially linking otherwise separate datasets and generating multiple iterations. Careful nomenclature of such files, as well as stoarge capacity seem very important considerations\nQuestions:\nWe don\u0026rsquo;t really have a way of doing write-then-read at the moment - would this be a good reason to adopt local SQLite (or other) registry?\nHow do we think about multiple runs with potentially thousands of outputs and encapsulate them in a single larger-scale output? Would RO Crate help here?\nUse Case 6. Flag and Track issues in data and model/analysis-code # Subsequent revisions to the inference algorithm (step-2), data used for inference (step-1) or additional data used for simulation (step-3) would then flag subsequent model outputs as needing revision. If outputs from step-4 are used in further analysis want to track dependencies i.e. if the data or analysis in any step is flagged I want to see this when I look at downstream outputs.\nThis requires application of four further component use cases shown below:\nUse case: Flag data as problematic \u0026ndash; an error or revision of data is uncovered. How do I flag this in data pipeline/registry?\nUse case: Flag model/analysis code as problematic \u0026ndash; a model/analysis code error is detected or the code revised. How do I flag this in data pipeline/registry?\nUse case: Track issues \u0026ndash; I am looking at the output of some analysis/model. How do I see issues flagged in ancestral data or model/analyisis?\nUse case: View meta data \u0026ndash; I want to extract meta data about a particular dataset or model code\nActor: Data/code producer (5 \u0026amp; 6). Data/software user (7 \u0026amp; 8)\nBenefit: Results are linked to any known issues with their dependencies, and potential users are aware of any known issues\nQuestion:\nAre models/analysis in the data pipeline/registry? - Yes, linked to particular Github hashes\nHow do we implement tracing flagged items through the dependency links and alerting users of them?\nHow are problems flagged / quality assessments recorded? Issues can be attached to any object in the registry (data products, software releases, ...). Can also have active quality assessments recorded (eg scorecards completed, data quality assessments) but should not rely on that being done for every object in the chain.\nUse Case 7. Production of a pipeline validated policy briefing # Actor: Modeller who has some data files pre-registered, and a policy request for information that requires a rerun of model code.\nPreconditions: Actor has an account (if required) to access data registry, local copies of any further datasets they need to upload, and a valid model code release registered.\nBasic Flow:\nActor collates/reviews list of data resources required for model run\nActor identifies any gaps in data and uploads local copies to registry\nActor interrogates registry to review status of all prerequisite data resources\nActor iterates choice of data resources, preferably to remove \u0026lsquo;at risk\u0026rsquo; selections, and to make time specific selections as they see fit\nActor interrogates registry to confirm status of all prerequisite data resources\nActor runs registered. released version of model software using selected data resources (likely to be multiple runs)\nOutput from model uploaded to registry\nModel output used in rmd-type framework to generate policy briefing\nPolicy briefing uploaded to registry with links to dependencies\nStatus reports for any given model run and for the entire the briefing can be generated (listing dependencies, provenance, flagging up QA status and issues)\n[Status of policy briefing reviewed prior to circulation]\n[Brief either circulated with appropriate cover based on meta-data, or rejected and referred back to be re-generated]\nPostconditions:\nNewly uploaded data products are in registry with associated metadata\nModel output and policy brief are in registry with associated metadata\nActor has a brief with known provenance ready for onward circulation\nQuestions:\nAgain \u0026ndash; what do we do with the data types (like the reports) that are not in a core data format?\nCould provenance reports and QA info be generated for any \u0026ldquo;object\u0026rdquo; in the registry?\nUse Case 8. Finding and assessing alternative datasets from registry # Actor: Modeller who has data files pre-registered, and who needs to decide which data resources to use in an output run\nPreconditions: Actor has an account (if required) to access data registry\nBasic Flow:\nActor collates list of data resources required for model run\nActor interrogates registry to review status of all prerequisite data resources\nWhere a data resource is flagged as \u0026lsquo;at risk\u0026rsquo;, or has some other property (too recent? Might want to roll back to an earlier version) actor can request a list, with summary metadata (tbc), of other \u0026lsquo;views/components/distributions\u0026rsquo; or versions of that data resource\nActor selects view for use in this run\nPostconditions:\nThe actor has been able to make an informed choice of data resource views Questions: Do we want to flag up data status/issues via web interface and also during programmatic access?\nUse Case 9. Review of current status of a pipeline validated output # Actor: Policy broker who has an output, previously generated and registered within the pipeline. There is a need to assess whether the output has been superseded, and if so, in what ways. Or modeller has an analysis that they have previously done, wants to know if they ran it again are any of the data products or model code updated?\nPreconditions: Actor has an account (if required) to access data registry.\nBasic Flow:\nActor finds code run or data output to investigate\nActor accesses registry entry for the specified output\nActor queries registry to establish the status of the data resources relative to the current state of the registry\nRegistry produces list of pre-requisite data resources, and (optionally) the status of these at the time the output was generated (I assume these latter are available from the metadata of the output itself)\nPostconditions:\nNo change to registry\nActor has sufficient information to assess the output to assess whether the data is\nUp to date\nIs there a newer version of any of the data or model code?\nHas any of the existing data now got attached warnings / invalidated\nActor has sufficient information to have an informed discussion with modeller as to the current validity of the past output\nUse Case 10. Retrospective assessment of status, when generated, of a pipeline validated output # Actor: Policy broker who has a query about the provenance of an output, previously generated and registered within the pipeline. There is a need to confirm whether the output had appropriate caveats attached when circulated. (Sorry to be negative, but I can see this happening\u0026hellip; )\nPreconditions: Actor has an account (if required) to access data registry.\nBasic Flow:\nActor accesses registry entry for the specified output\nActor queries registry to recover the status of the data resources used at the time the output was generated (I assume these latter are available from the metadata of the output itself; if these are not available, is there sufficient information in the metadata for them to be reconstructed?)\nPostconditions:\nNo change to registry\nActor has sufficient information to assess the output from the point of view of the needs of policy customers\nException cases:\nThe obvious one is the situation where the output has not been registered in the registry, but where there is sufficient knowledge of the data resources used to be confident about what was actually used. However, I think we should not seek to cover this scenario, since the users have already broken the system, and the existence of a tool to retrospectively cover everyone\u0026rsquo;s back might just encourage poor behaviour.. Questions: How would we keep track of history of status/issues to be able to re-create status reports from past points in time? If status consists of timestamped issues and QA reports that are then immutable, then this would be possible.\nHow important is it that this is easy / convenient vs \u0026ldquo;possible by digging into the database\u0026rdquo;?\nUse Case 11. Understanding of local data # Actor: Modeller has datasets locally on computer, wants to know if they are already on registry, and if so what they are\nPreconditions: Actor has an account (if required) to access data registry.\nBasic Flow:\nActor finds file / files to investigate\nActor queries registry to recover the status of the data\nPostconditions:\nNo change to registry\nActor has sufficient information to assess the output to assess whether the data is\nIn registry at all (from checksum)\nUp to date \u0026ndash; is there a newer version?\nHas attached warnings / invalidated\nException cases:\nWhat if the data was in the registry but has been corrupted or accidentally edited, is there some way of telling what it (probably?) should have been? Notes: Possible a tool for data import to the registry that calculates checksums could be run over the directory\nUse Case 12. Actor wants to understand provenance of reported results # Similar to use cases 9 or 10, but no preconditions about accounts, and actor may not have access to report, or may only have document\nActor: Member of public wants to understand provenance of reported results\nBasic Flow:\nPossible ways this could work:\n1. Via a searchable interface to the data registry\n2. Have access to PDF (or other format) of policy document\ncan use PDF directly to identify provenance report to display 2. Have access to pdf of report and can use pdf to identify provenance report to display\nPDF has embedded DOI or link to provenance report in registry\nPDF is uploaded by user and the system identifies it via a checksum\nPDF has a unique title/identifier used to look up provenance report\nNotes: Report documents must be (manually?) added to registry for this to work. Or must be part of workflow for latex document creation.\nUse Case 13. Actor wants to identify changes between model outputs # Any user of the model outputs (e.g. modeller, policy maker) wants to know what changed between two published model outputs / reports, and generates a diff between the provenances\nBasic Flow:\nThis requires use case 12, plus a mechanism for displaying a comprehensible diff.\nUse Case 14. From a screen on the visualisation platform, the user wants to follow a link to a provenance report for the underlying data and processing # Possible ways this could work:\n1. visualisation platform itself uses traceability framework within it to generate provenance reports\n2. visualisation platform manually links back to provenance of the components that are interactively selected\nUse Case 15. Initial quick look at dataset visualisations online # Any user (policy maker, modeller, etc.) may either want to investigate public datasets registered in data registry visually, or may be investigating a provenance reports and want to see what a dataset is that is traced in report.\nBasic flow:\nLinks directly from data pipeline to visualisation infrastructure, no programming\nUse Case 16. Actor wants to do a model run inside a safe haven # Preconditions: Actor has an account (if required) to access data registry, actor has access to safe haven, metadata is public\nBasic Flow:\nPossible ways this could work:\n1. Registry sits inside safe haven, as does all data required for analysis \u0026ndash; full pipeline runs inside safe haven\nRegistry must sync with an external registry (at simplest just a read-only copy) or have a direct external public interface to ensure public availability of provenance data, otherwise many use cases will break \u0026ndash; especially anything policy- or public-facing, where users will not have access to safe haven 2. Registry sits outside safe haven, public data can be hosted outside safe haven, but is pulled in as required to be used in the modelling run along with the private data. Pipeline run generates log files that are then pushed back to registry after run.\nThen as use case 4.\nNotes:\nPotential issue with 1 \u0026ndash; how does post-processing of results generated in a safe haven (e.g. to produce reports) work if write access to registry is only available inside safe haven?\nUse Case 17. Actor wants to run a model with some non-publicly accessible data # Preconditions: Actor has an account (if required) to access data registry, metadata is public, actor has access to the private data which is registered in registry\nBasic Flow:\nHow this could work:\nLocal pipeline scripts have to know about local cache of private files, then pipeline can execute as normal in use case 4.\n"
      }
      ,"18": {
        "url": "/docs/API/Java/coderun/",
        "relUrl": "/docs/API/Java/coderun/",
        "title": "Coderun",
        "doc": "Coderun",
        "section": "Java",
        "content": " javaDataPipeline Coderun # The main class used to interface with the FAIR DataPipeline in Java, is the Coderun class.\nUsers should initialise the Coderun instance using a try-with-resources block or ensure that .close() is explicitly called upon finishing.\nThe Coderun constructor needs a Path to the config.yaml, a Path to the script.sh, and the registry authentication token.\nThe user then would access the input data product(s) using coderun.get_dp_for_read(dataproduct_name) and output data product(s) using coderun.get_dp_for_write(dataproduct_name, extension).\nFrom the resulting data_product we can then access either one unnamed object_component: Object_component_read oc = dp.getComponent() or any number of named object_components: Object_component_read oc1 = dp.getComponent('ComponentName').\nData in FAIR Data Pipeline internal formats (HDF5, TOML) is written to (or read from) named Object_components, while other file formats (such as CSV) are written to (or read from) just 1 unnamed Object_component.\nHere is an example of a Coderun writing a CSV file to one unnamed Object_component:\ntry (var coderun = new Coderun(configPath, scriptPath, token)) { Data_product_write dp = coderun.get_dp_for_write(dataProduct, \u0026#34;csv\u0026#34;); Object_component_write oc = dp.getComponent(); Path p = oc.writeLink(); write_my_csv_data_to_path(p); } And for reading:\ntry (var coderun = new Coderun(configPath, scriptPath, token)) { Data_product_read dp = coderun.get_dp_for_read(dataProduct); Object_component_read oc = dp.getComponent(); Path p = oc.readLink(); read_my_csv_data_from_path(p); } Here is an example of a coderun writing Samples to one named Object_component:\ntry (var coderun = new Coderun(configPath, scriptPath)) { ImmutableSamples samples = ImmutableSamples.builder().addSamples(1, 2, 3).rng(rng).build(); String dataProduct = \u0026#34;animal/dodo\u0026#34;; String component1 = \u0026#34;example-samples-dodo1\u0026#34;; Data_product_write dp = coderun.get_dp_for_write(dataProduct, \u0026#34;toml\u0026#34;); Object_component_write oc1 = dp.getComponent(component1); oc1.writeSamples(samples); } "
      }
      ,"19": {
        "url": "/docs/API/R/",
        "relUrl": "/docs/API/R/",
        "title": "R",
        "doc": "R",
        "section": "Modelling APIs",
        "content": " rFDP # The rDataPipeline package contains functions used to interface with the FAIR Data Pipeline in R.\nTo install it, run:\ninstall.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;FAIRDataPipeline/rDataPipeline\u0026#34;) To view the package documentation, go here.\n"
      }
      ,"20": {
        "url": "/docs/SCRC/",
        "relUrl": "/docs/SCRC/",
        "title": "SCRC",
        "doc": "SCRC",
        "section": "Docs",
        "content": " The Scottish COVID-19 Response Consortium # Who are we? # The Scottish COVID-19 Response Consortium is formed of dozens of individuals from over 30 academic and commercial organisations.\nResearchers in these organisations jointly responded to a call by the Royal Society to develop more epidemiological models of COVID-19 spread - RAPID ASSISTANCE IN MODELLING THE PANDEMIC: RAMP - in order to develop a more robust and clearer understanding of the impacts of different exit strategies from lockdown. Scientists from several other organisations across the UK and abroad have now joined the consortium to provide additional expertise in specific areas.\nOur outputs # See our SCRC GitHub website for an overview of our work beyond the data pipeline, and our SCRC and FAIRDataPipeline GitHub organisations for our specific outputs.\n"
      }
      ,"21": {
        "url": "/docs/API/Java/issues/",
        "relUrl": "/docs/API/Java/issues/",
        "title": "Issues",
        "doc": "Issues",
        "section": "Java",
        "content": " javaDataPipeline Issues # Issues can be attached to Object_components, as well as to the Submission Script, Config File, and Code Repo.\nIssues can be created and then linked to the elements that they apply to, or they can be raised directly on the element it refers to.\nHere is an example of the creation then linking of an Issue to components, Script, and Code Repo:\ntry (Coderun coderun = new Coderun(configPath, scriptPath, token)) { Data_product_read dp = coderun.get_dp_for_read(\u0026#34;existing/dpname\u0026#34;); Object_component_read oc1 = dp.getComponent(\u0026#34;comp1\u0026#34;); Object_component_read oc2 = dp.getComponent(\u0026#34;comp2\u0026#34;); Issue i = coderun.raise_issue(\u0026#34;A serious issue\u0026#34;, 10); i.add_components(oc1, oc2); i.add_fileObject(coderun.getScript(), coderun.getCode_repo()); } And here is an example of raising an issue directly with a component and a Script:\ntry (Coderun coderun = new Coderun(configPath, scriptPath, token)) { Data_product_read dp = coderun.get_dp_for_read(\u0026#34;existing/dpname\u0026#34;); Object_component_read oc1 = dp.getComponent(\u0026#34;comp1\u0026#34;); oc1.raise_issue(\u0026#34;A serious issue\u0026#34;, 10); coderun.getScript().raise_issue(\u0026#34;Another pretty serious issue\u0026#34;, 9); } "
      }
      ,"22": {
        "url": "/docs/API/julia/",
        "relUrl": "/docs/API/julia/",
        "title": "Julia",
        "doc": "Julia",
        "section": "Modelling APIs",
        "content": " DataRegistryUtils.jl # The DataPipeline.jl package contains functions used to interface with the FAIR Data Pipeline in Julia.\nFeatures # Conveniently download Data Products from the SCRC Data Registry. File hash-based version checking: new data is downloaded only when necessary. A SQLite layer for convenient pre-processing (typically aggregation, and the joining of disparate datasets based on common identifiers.) Easily register model code or realisations (i.e. \u0026lsquo;runs\u0026rsquo;) with a single line of code. Installation # The package is not yet registered and must be added via the package manager Pkg:\nusing Pkg Pkg.add(url=\u0026#34;https://github.com/FAIRDataPipeline/DataPipeline.jl\u0026#34;) Usage # See the package documentation for instructions and examples.\nSource code # See the package\u0026rsquo;s code repo.\n"
      }
      ,"23": {
        "url": "/docs/cli/example0/",
        "relUrl": "/docs/cli/example0/",
        "title": "DPAPI examples",
        "doc": "DPAPI examples",
        "section": "Fair CLI",
        "content": " DPAPI examples # Note that this is a living document and the following is subject to change. This page gives simple examples of the user written config.yaml file alongside the working config file generated by FAIR run. Note that the Data Pipeline API will take the working config file as an input.\nEmpty code run # User written config.yaml # run_metadata: description: An empty code run local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/FAIRDataPipeline/FDP_validation/ script: |- R -f simple_working_examples/empty_script.R ${{CONFIG_DIR}} Working config.yaml # fair run should create a working config.yaml file, which is read by the Data Pipeline API. In this example, the working config.yaml file is pretty much identical to the original config.yaml file, only ${{CONFIG_DIR}} is replaced by the directory in which the working config.yaml file resides.\nrun_metadata: description: An empty code run local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/FAIRDataPipeline/FDP_validation/ script: |- R -f simple_working_examples/empty_script.R /Users/SoniaM/datastore/coderun/20210511-231444/ latest_commit: 221bfe8b52bbfb3b2dbdc23037b7dd94b49aaa70 remote_repo: https://github.com/FAIRDataPipeline/FDP_validation Submission script (R) # library(rDataPipeline) # Initialise Code Run config \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;config.yaml\u0026#34;) script \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;script.sh\u0026#34;) handle \u0026lt;- initialise(config, script) finalise(handle) Write data product (HDF5) # User written config.yaml # run_metadata: description: Write an array local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/FAIRDataPipeline/FDP_validation/ script: |- R -f simple_working_examples/write_array.R ${{CONFIG_DIR}} write: - data_product: test/array description: test array with simple data Working config.yaml # fdp run should create a working config.yaml file, which is read by the Data Pipeline API. In this example, the working config.yaml file is pretty much identical to the original config.yaml file, only ${{CONFIG_DIR}} is replaced by the directory in which the working config.yaml file resides.\nrun_metadata: description: Write an array local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/FAIRDataPipeline/FDP_validation/ script: |- R -f simple_working_examples/write_array.R /Users/SoniaM/datastore/coderun/20210511-231444/ public: true latest_commit: 221bfe8b52bbfb3b2dbdc23037b7dd94b49aaa70 remote_repo: https://github.com/FAIRDataPipeline/FDP_validation write: - data_product: test/array description: test array with simple data use: version: 0.1.0 Note that, although use: is reserved for aliasing in the user-written config, for simplicity the CLI will always write version here.\nNote also that by default, the CLI will write public: true to run_metadata:. The user is however free to specify public: false for individual writes.\nSubmission script (R) # library(rDataPipeline) # Initialise Code Run config \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;config.yaml\u0026#34;) script \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;script.sh\u0026#34;) handle \u0026lt;- initialise(config, script) df \u0026lt;- data.frame(a = 1:2, b = 3:4) rownames(df) \u0026lt;- 1:2 write_array(array = as.matrix(df), handle = handle, data_product = \u0026#34;test/array\u0026#34;, component = \u0026#34;component1/a/s/d/f/s\u0026#34;, description = \u0026#34;Some description\u0026#34;, dimension_names = list(rowvalue = rownames(df), colvalue = colnames(df)), dimension_values = list(NA, 10), dimension_units = list(NA, \u0026#34;km\u0026#34;), units = \u0026#34;s\u0026#34;) finalise(handle) Read data product (HDF5) # User written config.yaml # run_metadata: description: Read an array local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/FAIRDataPipeline/FDP_validation/ script: |- R -f simple_working_examples/read_array.R ${{CONFIG_DIR}} read: - data_product: test/array Working config.yaml # fdp run should create a working config.yaml file, which is read by the Data Pipeline API. In this example, the working config.yaml file is pretty much identical to the original config.yaml file, only ${{CONFIG_DIR}} is replaced by the directory in which the working config.yaml file resides.\nrun_metadata: description: Read an array local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/FAIRDataPipeline/FDP_validation/ script: |- R -f simple_working_examples/read_array.R /Users/SoniaM/datastore/coderun/20210511-231444/ latest_commit: 221bfe8b52bbfb3b2dbdc23037b7dd94b49aaa70 remote_repo: https://github.com/FAIRDataPipeline/FDP_validation read: - data_product: test/array use: version: 0.1.0 Submission script (R) # library(rDataPipeline) # Open the connection to the local registry with a given config file config \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;config.yaml\u0026#34;) script \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;script.sh\u0026#34;) handle \u0026lt;- initialise(config, script) data_product \u0026lt;- \u0026#34;test/array\u0026#34; component \u0026lt;- \u0026#34;component1/a/s/d/f/s\u0026#34; dat \u0026lt;- read_array(handle = handle, data_product = data_product, component = component) finalise(handle) Write data product (csv) # User written config.yaml # run_metadata: description: Write csv file local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/FAIRDataPipeline/FDP_validation/ script: |- R -f simple_working_examples/write_csv.R ${{CONFIG_DIR}} write: - data_product: test/csv description: test csv file with simple data file_type: csv Working config.yaml # run_metadata: description: Write csv file local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/FAIRDataPipeline/FDP_validation/ script: |- R -f simple_working_examples/write_csv.R /Users/SoniaM/datastore/coderun/20210511-231444/ public: true latest_commit: 221bfe8b52bbfb3b2dbdc23037b7dd94b49aaa70 remote_repo: https://github.com/FAIRDataPipeline/FDP_validation write: - data_product: test/csv description: test csv file with simple data file_type: csv use: version: 0.0.1 Submission script (R) # library(rDataPipeline) # Open the connection to the local registry with a given config file config \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;config.yaml\u0026#34;) script \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;script.sh\u0026#34;) handle \u0026lt;- initialise(config, script) df \u0026lt;- data.frame(a = 1:2, b = 3:4) rownames(df) \u0026lt;- 1:2 path \u0026lt;- link_write(handle, \u0026#34;test/csv\u0026#34;) write.csv(df, path) finalise(handle) Read data product (csv) # User written config.yaml # run_metadata: description: Read csv file local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/FAIRDataPipeline/FDP_validation/ script: |- R -f simple_working_examples/read_csv.R ${{CONFIG_DIR}} read: - data_product: test/csv Working config.yaml # run_metadata: description: Read csv file local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/FAIRDataPipeline/FDP_validation/ script: |- R -f simple_working_examples/read_csv.R /Users/SoniaM/datastore/coderun/20210511-231444/ latest_commit: 221bfe8b52bbfb3b2dbdc23037b7dd94b49aaa70 remote_repo: https://github.com/FAIRDataPipeline/FDP_validation read: - data_product: test/csv use: version: 0.0.1 Submission script (R) # library(rDataPipeline) # Open the connection to the local registry with a given config file config \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;config.yaml\u0026#34;) script \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;script.sh\u0026#34;) handle \u0026lt;- initialise(config, script) path \u0026lt;- link_read(handle, \u0026#34;test/csv\u0026#34;) df \u0026lt;- read.csv(path) finalise(handle) Write data product (point estimate) # User written config.yaml # run_metadata: description: Write point estimate local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/SCRC/SCRCdata/ script: |- R -f simple_working_examples/write_point_estimate.R ${{CONFIG_DIR}} write: - data_product: test/estimate/asymptomatic-period description: asymptomatic period Working config.yaml # run_metadata: description: Write point estimate local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/SCRC/SCRCdata/ script: |- R -f simple_working_examples/write_point_estimate.R /Users/SoniaM/datastore/coderun/20210511-231444/ public: true latest_commit: 221bfe8b52bbfb3b2dbdc23037b7dd94b49aaa70 remote_repo: https://github.com/FAIRDataPipeline/FDP_validation write: - data_product: test/estimate/asymptomatic-period description: asymptomatic period use: version: 0.0.1 Submission script (R) # library(rDataPipeline) # Open the connection to the local registry with a given config file config \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;config.yaml\u0026#34;) script \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;script.sh\u0026#34;) handle \u0026lt;- initialise(config, script) write_estimate(value = 9, handle = handle, data_product = \u0026#34;test/distribution/asymptomatic-period\u0026#34;, component = \u0026#34;asymptomatic-period\u0026#34;, description = \u0026#34;asymptomatic period\u0026#34;) finalise(handle) Read data product (point estimate) # User written config.yaml # run_metadata: description: Read point estimate local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/SCRC/SCRCdata/ script: |- R -f simple_working_examples/read_point_estimate.R ${{CONFIG_DIR}} read: - data_product: test/estimate/asymptomatic-period Working config.yaml # run_metadata: description: Read point estimate local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/SCRC/SCRCdata/ script: |- R -f simple_working_examples/read_point_estimate.R /Users/SoniaM/datastore/coderun/20210511-231444/ latest_commit: 221bfe8b52bbfb3b2dbdc23037b7dd94b49aaa70 remote_repo: https://github.com/FAIRDataPipeline/FDP_validation read: - data_product: test/estimate/asymptomatic-period use: version: 0.0.1 Submission script (R) # library(rDataPipeline) # Open the connection to the local registry with a given config file config \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;config.yaml\u0026#34;) script \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;script.sh\u0026#34;) handle \u0026lt;- initialise(config, script) read_estimate(handle = handle, data_product = \u0026#34;test/distribution/asymptomatic-period\u0026#34;, component = \u0026#34;asymptomatic-period\u0026#34;) finalise(handle) Write data product (distribution) # User written config.yaml # run_metadata: description: Write distribution local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/SCRC/SCRCdata/ script: |- R -f simple_working_examples/write_distribution.R ${{CONFIG_DIR}} write: - data_product: test/distribution/symptom-delay description: Estimate of symptom delay Working config.yaml # run_metadata: description: Write distribution local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/SCRC/SCRCdata/ script: |- R -f simple_working_examples/write_distribution.R /Users/SoniaM/datastore/coderun/20210511-231444/ public: true latest_commit: 221bfe8b52bbfb3b2dbdc23037b7dd94b49aaa70 remote_repo: https://github.com/FAIRDataPipeline/FDP_validation write: - data_product: test/distribution/symptom-delay description: Estimate of symptom delay use: version: 0.0.1 Submission script (R) # library(rDataPipeline) # Open the connection to the local registry with a given config file config \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;config.yaml\u0026#34;) script \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;script.sh\u0026#34;) handle \u0026lt;- initialise(config, script) write_distribution(handle = handle, data_product = \u0026#34;test/distribution/symptom-delay\u0026#34;, component = \u0026#34;symptom-delay\u0026#34;, distribution = \u0026#34;Gaussian\u0026#34;, parameters = list(mean = -16.08, SD = 30), description = \u0026#34;symptom delay\u0026#34;) finalise(handle) Read data product (distribution) # User written config.yaml # run_metadata: description: Read distribution local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/SCRC/SCRCdata/ script: |- R -f simple_working_examples/read_distribution.R ${{CONFIG_DIR}} read: - data_product: test/distribution/symptom-delay Working config.yaml # run_metadata: description: Read distribution local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/SCRC/SCRCdata/ latest_commit: 221bfe8b52bbfb3b2dbdc23037b7dd94b49aaa70 remote_repo: https://github.com/FAIRDataPipeline/FDP_validation script: |- R -f simple_working_examples/read_distribution.R /Users/SoniaM/datastore/coderun/20210511-231444/ latest_commit: 221bfe8b52bbfb3b2dbdc23037b7dd94b49aaa70 remote_repo: https://github.com/FAIRDataPipeline/FDP_validation read: - data_product: test/distribution/symptom-delay use: version: 0.0.1 Submission script (R) # library(rDataPipeline) # Open the connection to the local registry with a given config file config \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;config.yaml\u0026#34;) script \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;script.sh\u0026#34;) handle \u0026lt;- initialise(config, script) read_distribution(handle = handle, data_product = \u0026#34;test/distribution/symptom-delay\u0026#34;, component = \u0026#34;symptom-delay\u0026#34;) finalise(handle) Attach issue to component # User written config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/attach_issue.R ${{CONFIG_DIR}} write: - data_product: test/array/issues/component description: a test array Working config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/attach_issue.R /Users/SoniaM/datastore/coderun/20210511-231444/ public: true latest_commit: e3c0ebdf5ae079bd72f601ec5eefdf998c4fc8ec remote_repo: https://github.com/fake_org/fake_repo read: [] write: - data_product: test/array/issues/component description: a test array use: version: 0.1.0 Submission script (R) # In R, we can attach issues to components in different ways. If there\u0026rsquo;s a more elegant way to do this, please tell me!\nAttach an issue on the fly by referencing an index in the handle:\nlibrary(rDataPipeline) # Initialise Code Run config \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;config.yaml\u0026#34;) script \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;script.sh\u0026#34;) handle \u0026lt;- initialise(config, script) df \u0026lt;- data.frame(a = 1:2, b = 3:4) rownames(df) \u0026lt;- 1:2 component_id \u0026lt;- write_array(array = as.matrix(df), handle = handle, data_product = \u0026#34;test/array/issues/component\u0026#34;, component = \u0026#34;component1/a/s/d/f/s\u0026#34;, description = \u0026#34;Some description\u0026#34;, dimension_names = list(rowvalue = rownames(df), colvalue = colnames(df)), dimension_values = list(NA, 10), dimension_units = list(NA, \u0026#34;km\u0026#34;), units = \u0026#34;s\u0026#34;) issue \u0026lt;- \u0026#34;some issue\u0026#34; severity \u0026lt;- 7 raise_issue(index = component_id, handle = handle, issue = issue, severity = severity) finalise(handle) Attaching an issue to a data product that already exists in the data registry by referencing it explicitly:\nlibrary(rDataPipeline) # Initialise Code Run config \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;config.yaml\u0026#34;) script \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;script.sh\u0026#34;) handle \u0026lt;- initialise(config, script) issue \u0026lt;- \u0026#34;some issue\u0026#34; severity \u0026lt;- 7 raise_issue(handle = handle, data_product = \u0026#34;test/array/issues/component\u0026#34;, component = \u0026#34;component1/a/s/d/f/s\u0026#34;, version = \u0026#34;0.1.0\u0026#34;, namespace = \u0026#34;username\u0026#34;, issue = issue, severity = severity) finalise(handle) Attaching an issue to multiple components at the same time:\nraise_issue(index = c(component_id1, component_id2), handle = handle, issue = issue, severity = severity) or\nraise_issue(handle = handle, data_product = \u0026#34;test/array/issues/component\u0026#34;, component = c(\u0026#34;component1/a/s/d/f/s\u0026#34;, \u0026#34;component2/a/s/d/f/s\u0026#34;), version = \u0026#34;0.1.0\u0026#34;, namespace = \u0026#34;username\u0026#34;, issue = issue, severity = severity) Attach issue to whole data product # User written config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/attach_issue.R ${{CONFIG_DIR}} write: - data_product: \u0026#34;test/array/issues/whole\u0026#34; description: a test array file_type: csv Working config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/attach_issue.R /Users/SoniaM/datastore/coderun/20210511-231444/ public: true latest_commit: 40725b40252fd55ba355f7ed66f5a42387f1674f remote_repo: https://github.com/fake_org/fake_repo read: [] write: - data_product: test/array/issues/whole description: a test array file_type: csv use: version: 0.1.0 Submission script (R) # In R, we can attach issues to data products in different ways. If there\u0026rsquo;s a more elegant way to do this, please tell me!\nAttach an issue on the fly by referencing an index in the handle:\nlibrary(rDataPipeline) # Initialise Code Run config \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;config.yaml\u0026#34;) script \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;script.sh\u0026#34;) handle \u0026lt;- initialise(config, script) df \u0026lt;- data.frame(a = 1:2, b = 3:4) rownames(df) \u0026lt;- 1:2 index \u0026lt;- write_array(array = as.matrix(df), handle = handle, data_product = \u0026#34;test/array/issues/whole\u0026#34;, component = \u0026#34;component1/a/s/d/f/s\u0026#34;, description = \u0026#34;Some description\u0026#34;, dimension_names = list(rowvalue = rownames(df), colvalue = colnames(df))) write_array(array = as.matrix(df), handle = handle, data_product = \u0026#34;test/array/issues/whole\u0026#34;, component = \u0026#34;component2/a/s/d/f/s\u0026#34;, description = \u0026#34;Some description\u0026#34;, dimension_names = list(rowvalue = rownames(df), colvalue = colnames(df))) issue \u0026lt;- \u0026#34;some issue\u0026#34; severity \u0026lt;- 7 raise_issue(index = index, handle = handle, issue = issue, severity = severity, whole_object = TRUE) finalise(handle) Attaching an issue to a data product that already exists in the data registry by referencing it explicitly:\nlibrary(rDataPipeline) # Initialise Code Run config \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;config.yaml\u0026#34;) script \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;script.sh\u0026#34;) handle \u0026lt;- initialise(config, script) issue \u0026lt;- \u0026#34;some issue\u0026#34; severity \u0026lt;- 7 raise_issue(handle = handle, data_product = \u0026#34;test/array/issues/whole\u0026#34;, version = \u0026#34;0.1.0\u0026#34;, namespace = \u0026#34;username\u0026#34;, issue = issue, severity = severity) finalise(handle) Attaching an issue to multiple data products at the same time:\nraise_issue(index = c(index1, index2), handle = handle, issue = issue, severity = severity, whole_object = TRUE) or\nraise_issue(handle = handle, data_product = c(\u0026#34;test/array/issues/whole\u0026#34;, \u0026#34;test/array/issues/whole/2\u0026#34;), version = c(\u0026#34;0.1.0\u0026#34;, \u0026#34;0.1.0\u0026#34;), namespace = \u0026#34;username\u0026#34;, issue = issue, severity = severity) Attach issue to config # User written config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/attach_issue.R ${{CONFIG_DIR}} Working config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/attach_issue.R /Users/SoniaM/datastore/coderun/20210511-231444/ latest_commit: 0d98e732b77e62a6cd390c6aec655f260f5f9b33 remote_repo: https://github.com/fake_org/fake_repo read: [] write: [] Submission script (R) # library(rDataPipeline) # Initialise Code Run config \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;config.yaml\u0026#34;) script \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;script.sh\u0026#34;) handle \u0026lt;- initialise(config, script) config_issue \u0026lt;- \u0026#34;issue with config\u0026#34; config_severity \u0026lt;- 7 raise_issue_config(handle = handle, issue = config_issue, severity = config_severity) finalise(handle) Attach issue to submission script # User written config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/attach_issue.R ${{CONFIG_DIR}} Working config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/attach_issue.R /Users/SoniaM/datastore/coderun/20210511-231444/ latest_commit: 358f64c4044f3b3f761865ee8e9f4375cf41d155 remote_repo: https://github.com/fake_org/fake_repo read: [] write: [] Submission script (R) # library(rDataPipeline) # Initialise Code Run config \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;config.yaml\u0026#34;) script \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;script.sh\u0026#34;) handle \u0026lt;- initialise(config, script) script_issue \u0026lt;- \u0026#34;issue with script\u0026#34; script_severity \u0026lt;- 7 raise_issue_script(handle = handle, issue = script_issue, severity = script_severity) finalise(handle) Attach issue to GitHub repository # User written config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/attach_issue.R ${{CONFIG_DIR}} Working config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/attach_issue.R /Users/SoniaM/datastore/coderun/20210511-231444/ latest_commit: 6b23ec822bfd7ea5f419c70ce18fb73b59c90754 remote_repo: https://github.com/fake_org/fake_repo read: [] write: [] Submission script (R) # library(rDataPipeline) # Initialise Code Run config \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;config.yaml\u0026#34;) script \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;script.sh\u0026#34;) handle \u0026lt;- initialise(config, script) repo_issue \u0026lt;- \u0026#34;issue with repo\u0026#34; repo_severity \u0026lt;- 7 raise_issue_repo(handle = handle, issue = repo_issue, severity = repo_severity) finalise(handle) Attach issue to external object # This is not something we want to do.\nAttach issue to code run # This might be something we want to do in the future, but not now.\nDelete DataProduct (optionally) if identical to previous version # Delete CodeRun (optionally) if nothing happened # That is, if no output was created and no issue was raised\nCodeRun with aliases (use block example) # User written config.yaml # run_metadata: description: A test model local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: SCRC default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/SCRC/SCRCdata script: |- R -f inst/SCRC/scotgov_management/submission_script.R ${{CONFIG_DIR}} read: - data_product: test/data/alias use: namespace: johnsmith data_product: scotland/human/population write: - data_product: human/outbreak-timeseries description: data product description use: data_product: scotland/human/outbreak-timeseries - data_product: human/outbreak/simulation_run description: another data product description use: data_product: human/outbreak/simulation_run-${{RUN_ID}} Working config.yaml # fair run should create a working config.yaml file, which is read by the Data Pipeline API. In this example, the working config.yaml file is pretty much identical to the original config.yaml file, only ${{CONFIG_DIR}} is replaced by the directory in which the working config.yaml file resides.\nrun_metadata: description: A test model local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ public: true local_repo: /Users/Soniam/Desktop/git/SCRC/SCRCdata latest_commit: 221bfe8b52bbfb3b2dbdc23037b7dd94b49aaa70 remote_repo: https://github.com/ScottishCovidResponse/SCRCdata script: |- R -f inst/SCRC/scotgov_management/submission_script.R /Users/SoniaM/datastore/coderun/20210511-231444/ read: - data_product: human/population use: data_product: scotland/human/population version: 0.1.0 namespace: johnsmith write: - data_product: human/outbreak-timeseries description: data product description use: data_product: scotland/human/outbreak-timeseries version: 0.1.0 - data_product: human/outbreak/simulation_run description: another data product description use: data_product: human/outbreak/simulation_run-${{RUN_ID}} version: 0.1.0 CodeRun with read globbing # This example makes use of globbing in the read: block.\nFirst we need to populate your local registry with something to read:\nUser written config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/input_globbing.R ${{CONFIG_DIR}} write: - data_product: real/data/1d06c1840618f1cd0ff29177b34fa68df939a9a8/1 description: A csv file file_type: csv - data_product: real/data/1d06c1840618f1cd0ff29177b34fa68df939a9a8/thing/1 description: A csv file file_type: csv Working config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/input_globbing.R /Users/SoniaM/datastore/coderun/20210511-231444/ public: yes latest_commit: 064e900b691e80058357a344f02cf73de0166fab remote_repo: https://github.com/fake_org/fake_repo read: [] write: - data_product: real/data/1d06c1840618f1cd0ff29177b34fa68df939a9a8/1 description: A csv file file_type: csv use: version: 0.0.1 - data_product: real/data/1d06c1840618f1cd0ff29177b34fa68df939a9a8/thing/1 description: A csv file file_type: csv use: version: 0.0.1 Now that our local registry is populated, we can try globbing:\nUser written config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/input_globbing.R ${{CONFIG_DIR}} read: - data_product: real/data/1d06c1840618f1cd0ff29177b34fa68df939a9a8/* Working config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/input_globbing.R /Users/SoniaM/datastore/coderun/20210511-231444/ latest_commit: b9e2187b3796f06ca33f92c3a82863215917ed0e remote_repo: https://github.com/fake_org/fake_repo read: - data_product: real/data/1d06c1840618f1cd0ff29177b34fa68df939a9a8/thing/1 use: version: 0.0.1 - data_product: real/data/1d06c1840618f1cd0ff29177b34fa68df939a9a8/1 use: version: 0.0.1 write: [] CodeRun with write globbing # This example makes use of globbing in the write: block.\nFirst we need to populate your local registry with some data:\nUser written config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/output_globbing.R ${{CONFIG_DIR}} write: - data_product: real/data/e8d7af00c8f8e24c2790e2a32241bc1bfc8cf011/1 description: A csv file file_type: csv - data_product: real/data/e8d7af00c8f8e24c2790e2a32241bc1bfc8cf011/thing/1 description: A csv file file_type: csv Working config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/output_globbing.R /Users/SoniaM/datastore/coderun/20210511-231444/ public: yes latest_commit: 2a8688677321b99e3a2545ce020992d136334b71 remote_repo: https://github.com/fake_org/fake_repo read: [] write: - data_product: real/data/e8d7af00c8f8e24c2790e2a32241bc1bfc8cf011/1 description: A csv file file_type: csv use: version: 0.0.1 - data_product: real/data/e8d7af00c8f8e24c2790e2a32241bc1bfc8cf011/thing/1 description: A csv file file_type: csv use: version: 0.0.1 Now that our local registry is populated, we can try globbing:\nUser written config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/output_globbing.R ${{CONFIG_DIR}} write: - data_product: real/data/e8d7af00c8f8e24c2790e2a32241bc1bfc8cf011/* description: A csv file file_type: csv use: version: ${{MAJOR}} Working config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: http://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: username default_output_namespace: username write_data_store: /Users/username/datastore/ local_repo: local_repo script: |- R -f simple_working_examples/output_globbing.R /Users/SoniaM/datastore/coderun/20210511-231444/ public: yes latest_commit: f95815976cd4d93c062f94a48525fcec88b6ef34 remote_repo: https://github.com/fake_org/fake_repo read: [] write: - data_product: real/data/e8d7af00c8f8e24c2790e2a32241bc1bfc8cf011/* description: A csv file file_type: csv use: version: 1.0.0 - data_product: real/data/e8d7af00c8f8e24c2790e2a32241bc1bfc8cf011/thing/1 description: A csv file file_type: csv use: version: 1.0.0 - data_product: real/data/e8d7af00c8f8e24c2790e2a32241bc1bfc8cf011/1 description: A csv file file_type: csv use: version: 1.0.0 ``` "
      }
      ,"24": {
        "url": "/docs/API/Java/parameters/",
        "relUrl": "/docs/API/Java/parameters/",
        "title": "Parameters",
        "doc": "Parameters",
        "section": "Java",
        "content": " Parameters # Below are examples of all the parameter types that can be written to and read from the FAIR Data Pipeline TOML parameter file format. These TOML parameter files are described on the API page.\nSamples\nsamples = ImmutableSamples.builder().addSamples(1, 2, 3).rng(rng).build(); object_component.writeSamples(samples); Distribution (Gamma)\ndistribution = ImmutableDistribution.builder() .internalShape(1) .internalScale(2) .internalType(DistributionType.gamma) .rng(rng) .build(); object_component.writeDistribution(distribution); Categorical Distribution\nMinMax firstMinMax = ImmutableMinMax.builder() .isLowerInclusive(true) .isUpperInclusive(true) .lowerBoundary(0) .upperBoundary(4) .build(); MinMax secondMinMax = ImmutableMinMax.builder() .isLowerInclusive(true) .isUpperInclusive(true) .lowerBoundary(5) .upperBoundary(9) .build(); MinMax thirdMinMax = ImmutableMinMax.builder() .isLowerInclusive(true) .isUpperInclusive(true) .lowerBoundary(10) .upperBoundary(14) .build(); MinMax fourthMinMax = ImmutableMinMax.builder() .isLowerInclusive(true) .isUpperInclusive(true) .lowerBoundary(15) .upperBoundary(20) .build(); categoricalDistribution = ImmutableDistribution.builder() .internalType(DistributionType.categorical) .bins(List.of(firstMinMax, secondMinMax, thirdMinMax, fourthMinMax)) .weights(List.of(0.4, 0.1, 0.1, 0.4)) .rng(rng) .build(); object_component.writeDistribution(categoricalDistribution); Estimate\nobject_component.writeEstimate(1.234); "
      }
      ,"25": {
        "url": "/docs/API/python/",
        "relUrl": "/docs/API/python/",
        "title": "Python",
        "doc": "Python",
        "section": "Modelling APIs",
        "content": " Standardised data type API # The standard API (or standardised data type API) is used to read/write arrays/tables/distributions/samples/estimates in python.\n"
      }
      ,"26": {
        "url": "/docs/API/Java/",
        "relUrl": "/docs/API/Java/",
        "title": "Java",
        "doc": "Java",
        "section": "Modelling APIs",
        "content": " javaDataPipeline # The javaDataPipeline Java library allows Java modellers to interface with the FAIR Data Pipeline. It is available on Maven Central:\ndependencies { implementation \u0026#39;org.fairdatapipeline:api:1.0.0-beta\u0026#39; } The main purpose of interfacing with the FAIR Data Pipeline is the recording of Coderuns: a Coderun is a recorded session of your modelling code, all the inputs used and outputs generated will be recorded and a Provenance Report will be generated.\nThe basic idea behind the FAIR DataPipeline and Coderuns is explained on the Data pipeline page.\nUsage # A useful starting point to get to know the javaDataPipeline is to explore the java Simple Model. This shows example config.yaml files, example main(String[] args) code that allows fair run to call your code, and example Coderun() code for reading/writing input/output Data Products.\nThe links below give a basic introduction to the javaDataPipeline; these should be read in conjunction with the javaDocs which give more precise specification of the java API.\nBasic Coderun\nIssues\nParameters\nHDF5\nRun\nDebug\nSee the package documentation for Javadoc.\nLicense/copyright # Copyright (c) 2021: Bram Boskamp, Biomathematics and Statistics Scotland and the Scottish COVID-19 Response Consortium\nGNU GENERAL PUBLIC LICENSE Version 3\nSource code # See the package\u0026rsquo;s code repo.\n"
      }
      ,"27": {
        "url": "/docs/API/Java/hdf5/",
        "relUrl": "/docs/API/Java/hdf5/",
        "title": "Hdf5",
        "doc": "Hdf5",
        "section": "Java",
        "content": " HDF5 # HDF files are not supported in the current version of the Java FAIR Data Pipeline API.\n"
      }
      ,"28": {
        "url": "/docs/API/Java/run/",
        "relUrl": "/docs/API/Java/run/",
        "title": "Run",
        "doc": "Run",
        "section": "Java",
        "content": " Running your code # Below is a brief explanation how to make use of the Java FAIRDataPipeline library and actually getting your code to run from the FAIR CLI command line interface. The actual command to run your Java code is given in the script: line in the config.yaml. In my case I run the Java code using gradle run --args \u0026quot;${{CONFIG_DIR}}\u0026quot;. You\u0026rsquo;ll have to change this if you don\u0026rsquo;t use gradle.\nBelow is an example config.yaml, stored in your projects /src/main/resources:\nrun_metadata: default_input_namespace: test description: Java test coderun script: | gradle run --args \u0026#34;${{CONFIG_DIR}}\u0026#34; write: - data_product: test/results/my-result description: test results Your Java code then needs to read the registry token from the environment variable, and create a coderun using the config.yaml and script.sh files that will be placed in the CONFIG_DIR by FAIR CLI:\nString token = System.getenv(\u0026#34;FDP_LOCAL_TOKEN\u0026#34;); Path configFile = Path.of(args[0]).resolve(\u0026#34;config.yaml\u0026#34;); Path scriptFile = Path.of(args[0]).resolve(\u0026#34;script.sh\u0026#34;); try(var coderun = new Coderun(configPath, scriptPath, token)) { // create DP, Object_component, write results; } From your project root you can then just run the FAIR CLI commands:\nfair init fair pull src/main/resources/config.yaml fair run src/main/resources/config.yaml "
      }
      ,"29": {
        "url": "/docs/API/Java/debug/",
        "relUrl": "/docs/API/Java/debug/",
        "title": "Debug",
        "doc": "Debug",
        "section": "Java",
        "content": " Debugging / troubleshooting # Logger # The javaDataPipeline library uses SLF4J (Simple Logging Facade for Java).\nYou can either use this with the SLF4J simple logger, or it can bind to your own favourite logging framework. In order to set the default simple logger to more verbose logging:\nadd the org.slf4j:slf4j-simple dependency set the log level: System.setProperty(org.slf4j.impl.SimpleLogger.DEFAULT_LOG_LEVEL_KEY, \u0026quot;TRACE\u0026quot;);\nRunning manually # Instead of running:\nfair run src/main/resources/config.yaml You can call fair run and your actual java code separately:\nfair run --ci src/main/resources/config.yaml gradle run --args \u0026#34;/tmp/tmpXXXXXXX/data_store/jobs/\u0026lt;timestamp\u0026gt;\u0026#34; The actual tmp location is shown as output from the fair run command.\nThis may help debug/troubleshoot.\n"
      }
      ,"30": {
        "url": "/docs/cli/config/",
        "relUrl": "/docs/cli/config/",
        "title": "config.yaml fields",
        "doc": "config.yaml fields",
        "section": "Fair CLI",
        "content": " config.yaml fields # Note that this is a living document and the following is subject to change.\nThe Data Pipeline API hinges on a config.yaml file, which lets users specify metadata to be used during file lookup for read or write, and configure overall API behaviour. This user written config.yaml is translated into a working config file by FAIR run, which is then taken as input by the Data Pipeline API.\nThis page gives examples of the user written config.yaml file.\nSimple inputs and outputs # The following example reads various pieces of data and writes an external object.\nrun_metadata: description: A simple analysis local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: SCRC default_output_namespace: johnsmith write_data_store: /datastore/ local_repo: /Users/johnsmith/git/myproject/ # `script:` points to the submission script (relative to local_repo) script: python path/submission_script.py ${{CONFIG_PATH}} # `script_path:` can be used instead of `script:` read: # Read version 1.0 of human/commutes - data_product: human/commutes version: 1.0 # Read human/health from the cache - data_product: human/health use: cache: /local/file.h5 # Read crummy_table with specific doi and title - external_object: crummy_table doi: 10.1111/ddi.12887 title: Supplementary Table 2 # Read secret_data with specific doi and title from the cache - external_object: secret_data doi: 10.1111/ddi.12887 title: Supplementary Table 3 use: cache: /local/secret.csv # Read weird_lost_file (which perhaps has no metadata) with specific hash - object: weird_lost_file hash: b5a514810b4cb6dc795848464572771f write: # Write beautiful_figure and increment version number - external_object: beautiful_figure unique_name: My amazing figure version: ${{MINOR}} public: false run_metadata: provides metadata for the run:\ndescription: is a human readable description of the purpose of the config.yaml local_data_registry_url: specifies the local data registry root, which defaults to https://localhost:8000/api/ remote_data_registry_url: specifies the remote data registry endpoint, which defaults to https://data.fairdatapipeline.org/api/ default_input_namespace: and default_output_namespace: specify the default namespace for reading and writing write_data_store: specifies the file system root used for data writes, which is set here to /datastore. Note that if a file is referenced in the local filesystem (files specified in read: use: cache:) but that part of the local filesystem is not within a StorageRoot that the registry knows about, then the file will be copied into the write_data_store so that it can be referenced correctly in the registry. The submission script itself should either be written in script or stored in a text file in script_path, which can be absolute or relative to local_repo: (the root of the local repository) Any other fields will be ignored read: and write: provide references to data:\ndata_product: (within read: and write:), external_object: (read: and write:) and object: (read: only) specify metadata subsets that are matched in the read and write processes. The metadata values may use glob syntax, in which case matching is done against the glob. For reads, a cache: may be specified directly, in which case it will be used without any further lookup. If a write is carried out to a data product where no such data_product: entry exists, then a new data product is created with that name in the local namespace, or the patch version of an existing data product is suitably incremented. The level of incrementation or version number can be explicitly defined by version:. If a write is carried out to an object that is not a data product and no such external_object: entry exists, then a new object is created with no associated external object or data product, and an issue is raised with the object to note the absence of an appropriate reference, referencing the name given in the write API call. version: can be specified explicitly (e.g. 0.1.0 or 0.20210414.0), by reference (e.g. 0.${{DATE}}.0, meaning 0.20210414.0), or by increment (i.e. ${{MAJOR}}, ${{MINOR}}, or ${{PATCH}}). If an object already exists and no version is specified, it will be incremented by patch, by default. public: can be specified for data products in write: and is taken to be true when absent Extended inputs and outputs # The following example registers a new external object and writes a data product component.\nrun_metadata: description: Register a file in the pipeline local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: SCRC default_output_namespace: johnsmith write_data_store: /datastore/ local_repo: /Users/johnsmith/git/myproject/ script: # Points to the Python script, below (relative to local_repo) python path/submission_script.py {CONFIG_PATH} # `script_path:` can be used instead of `script:` register: - external_object: records/SARS-CoV-2/scotland/human-mortality # Who owns the data? namespace_name: Scottish Government Open Data Repository namespace_full_name: Scottish Government Open Data Repository namespace_website: https://statistics.gov.scot/ # Where does the data come from? root: https://statistics.gov.scot/sparql.csv?query= path: |- PREFIX qb: \u0026lt;http://purl.org/linked-data/cube#\u0026gt; PREFIX data: \u0026lt;http://statistics.gov.scot/data/\u0026gt; PREFIX rdfs: \u0026lt;http://www.w3.org/2000/01/rdf-schema#\u0026gt; PREFIX dim: \u0026lt;http://purl.org/linked-data/sdmx/2009/dimension#\u0026gt; PREFIX sdim: \u0026lt;http://statistics.gov.scot/def/dimension/\u0026gt; PREFIX stat: \u0026lt;http://statistics.data.gov.uk/def/statistical-entity#\u0026gt; PREFIX mp: \u0026lt;http://statistics.gov.scot/def/measure-properties/\u0026gt; SELECT ?featurecode ?featurename ?areatypename ?date ?cause ?location ?gender ?age ?type ?count WHERE { ?indicator qb:dataSet data:deaths-involving-coronavirus-covid-19; mp:count ?count; qb:measureType ?measType; sdim:age ?value; sdim:causeOfDeath ?causeDeath; sdim:locationOfDeath ?locDeath; sdim:sex ?sex; dim:refArea ?featurecode; dim:refPeriod ?period. ?measType rdfs:label ?type. ?value rdfs:label ?age. ?causeDeath rdfs:label ?cause. ?locDeath rdfs:label ?location. ?sex rdfs:label ?gender. ?featurecode stat:code ?areatype; rdfs:label ?featurename. ?areatype rdfs:label ?areatypename. ?period rdfs:label ?date. } # Metadata title: Deaths involving COVID19 description: Nice description of the dataset unique_name: Scottish deaths involving COVID19 alternate_identifier_type: ods_name file_type: csv release_date: ${{DATETIME}} version: 0.${{DATE}}.0 primary: True write: - data_product: records/SARS-CoV-2/scotland/human-mortality/results description: human mortality data version: 0.${{DATE}}.0 register: will take exactly one of unique_name: and alternate_identifier_type, or identifier:. Flexible inputs and outputs # The following example describes an analysis which typically reads human/population and writes human/outbreak-timeseries. Instead, a test model is run using Scottish data, whereby scotland/human/population is read from the eera namespace, rather than human/population. Likewise, the output is written as scotland/human/outbreak-timeseries rather than human/outbreak-timeseries.\nrun_metadata: description: A test model local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: SCRC default_output_namespace: johnsmith write_data_store: /datastore/ local_repo: /Users/johnsmith/git/myproject/ script: # Points to the Python script, below (relative to local_repo) python path/submission_script.py {CONFIG_PATH} read: - data_product: human/population use: namespace: eera data_product: scotland/human/population write: - data_product: human/outbreak-timeseries use: data_product: scotland/human/outbreak-timeseries - data_product: human/outbreak/simulation_run use: data_product: human/outbreak/simulation_run-${{RUN_ID}} read: and write: provide references to data: The corresponding use: sections contain metadata that is used to update the call metadata before the file access is attempted Any part of a use: statement may contain the string ${{RUN_ID}}, which will be replaced with the run id, otherwise a hash of the config contents and the date will be used "
      }
      ,"32": {
        "url": "/docs/cli/example1/",
        "relUrl": "/docs/cli/example1/",
        "title": "DPAPI functions",
        "doc": "DPAPI functions",
        "section": "Fair CLI",
        "content": " DPAPI functions # Note that this is a living document and the following is subject to change. This page gives a full working example of the user written config.yaml file alongside the working config file generated by FAIR run. Note that the Data Pipeline API will take the working config file as an input.\nThe following example downloads some data from outside the pipeline, does some processing in R (for example), and records the original file and the resultant data product into the pipeline.\nIn this simple example, the user should run the following from the terminal:\nfair pull config.yaml fair run config.yaml fair push config.yaml These functions require a config.yaml file to be supplied by the user. This file should specify various metadata associated with the code run, including where external objects comes from and the aliases that will be used in the submission script, data objects to be read and written, and the submission scipt location.\nUser written config.yaml # run_metadata: description: Register a file in the pipeline local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: SCRC default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/SCRC/SCRCdata script: |- R -f inst/SCRC/scotgov_management/submission_script.R ${{CONFIG_DIR}} register: - external_object: records/SARS-CoV-2/scotland/cases-and-management namespace_name: Scottish Government Open Data Repository namespace_full_name: Scottish Government Open Data Repository namespace_website: https://statistics.gov.scot/ root: https://statistics.gov.scot/sparql.csv?query= path: | PREFIX qb: \u0026lt;http://purl.org/linked-data/cube#\u0026gt; PREFIX data: \u0026lt;http://statistics.gov.scot/data/\u0026gt; PREFIX rdfs: \u0026lt;http://www.w3.org/2000/01/rdf-schema#\u0026gt; PREFIX mp: \u0026lt;http://statistics.gov.scot/def/measure-properties/\u0026gt; PREFIX dim: \u0026lt;http://purl.org/linked-data/sdmx/2009/dimension#\u0026gt; PREFIX sdim: \u0026lt;http://statistics.gov.scot/def/dimension/\u0026gt; PREFIX stat: \u0026lt;http://statistics.data.gov.uk/def/statistical-entity#\u0026gt; SELECT ?featurecode ?featurename ?date ?measure ?variable ?count WHERE { ?indicator qb:dataSet data:coronavirus-covid-19-management-information; dim:refArea ?featurecode; dim:refPeriod ?period; sdim:variable ?varname; qb:measureType ?type. {?indicator mp:count ?count.} UNION {?indicator mp:ratio ?count.} ?featurecode \u0026lt;http://publishmydata.com/def/ontology/foi/displayName\u0026gt; ?featurename. ?period rdfs:label ?date. ?varname rdfs:label ?variable. ?type rdfs:label ?measure. } title: Data associated with COVID-19 description: The data provide past data around COVID-19 for the daily updates provided by the Scottish Government. unique_name: COVID-19 management information file_type: csv release_date: ${{DATETIME}} version: 0.${{DATE}}.0 primary: True write: - data_product: records/SARS-CoV-2/scotland/cases-and-management/ambulance description: Ambulance data version: 0.${{DATE}}.0 - data_product: records/SARS-CoV-2/scotland/cases-and-management/calls description: Calls data version: 0.${{DATE}}.0 - data_product: records/SARS-CoV-2/scotland/cases-and-management/carehomes description: Care homes data version: 0.${{DATE}}.0 - data_product: records/SARS-CoV-2/scotland/cases-and-management/hospital description: Hospital data version: 0.${{DATE}}.0 - data_product: records/SARS-CoV-2/scotland/cases-and-management/mortality description: Mortality data version: 0.${{DATE}}.0 - data_product: records/SARS-CoV-2/scotland/cases-and-management/nhsworkforce description: NHS workforce data version: 0.${{DATE}}.0 - data_product: records/SARS-CoV-2/scotland/cases-and-management/schools description: Schools data version: 0.${{DATE}}.0 - data_product: records/SARS-CoV-2/scotland/cases-and-management/testing description: Testing data version: 0.${{DATE}}.0 Working config.yaml # fair run should create a working config.yaml file, which is then read by the Data Pipeline API.\nrun_metadata: description: Register a file in the pipeline local_data_registry_url: https://localhost:8000/api/ remote_data_registry_url: https://data.fairdatapipeline.org/api/ default_input_namespace: soniamitchell default_output_namespace: soniamitchell write_data_store: /Users/SoniaM/datastore/ local_repo: /Users/Soniam/Desktop/git/SCRC/SCRCdata script: |- R -f inst/SCRC/scotgov_management/submission_script.R /Users/SoniaM/datastore/coderun/20210511-231444/ public: true latest_commit: 221bfe8b52bbfb3b2dbdc23037b7dd94b49aaa70 remote_repo: https://github.com/ScottishCovidResponse/SCRCdata read: - data_product: records/SARS-CoV-2/scotland/cases-and-management use: data_product: records/SARS-CoV-2/scotland/cases-and-management version: 0.20210414.0 namespace: soniamitchell write: - data_product: records/SARS-CoV-2/scotland/cases-and-management/ambulance description: Ambulance data use: version: 0.20210414.0 - data_product: records/SARS-CoV-2/scotland/cases-and-management/calls description: Calls data use: version: 0.20210414.0 - data_product: records/SARS-CoV-2/scotland/cases-and-management/carehomes description: Care homes data use: version: 0.20210414.0 - data_product: records/SARS-CoV-2/scotland/cases-and-management/hospital description: Hospital data use: version: 0.20210414.0 - data_product: records/SARS-CoV-2/scotland/cases-and-management/mortality description: Mortality data use: version: 0.20210414.0 - data_product: records/SARS-CoV-2/scotland/cases-and-management/nhsworkforce description: NHS workforce data use: version: 0.20210414.0 - data_product: records/SARS-CoV-2/scotland/cases-and-management/schools description: Schools data use: version: 0.20210414.0 - data_product: records/SARS-CoV-2/scotland/cases-and-management/testing description: Testing data use: version: 0.20210414.0 submission_script.R # A submission script should be supplied by the user, which in this case registers an external object, reads it in, and then writes it back to the pipeline as a data product component. In the above example, this script is located in \u0026lt;local_repo\u0026gt;/inst/SCRC/scotgov_management/submission_script.R.\nlibrary(SCRCdataAPI) # Open the connection to the local registry with a given config file config \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;config.yaml\u0026#34;) script \u0026lt;- file.path(Sys.getenv(\u0026#34;FDP_CONFIG_DIR\u0026#34;), \u0026#34;script.sh\u0026#34;) handle \u0026lt;- initialise(config, script) # Return location of file stored in the pipeline input_path \u0026lt;- link_read(handle, \u0026#34;records/SARS-CoV-2/scotland/cases-and-management/mortality\u0026#34;) # Process raw data and write data product data \u0026lt;- read.csv(input_path) array \u0026lt;- some_processing(data) # e.g. data wrangling, running a model, etc. index \u0026lt;- write_array(array, handle, data_product = \u0026#34;records/SARS-CoV-2/scotland/cases-and-management/mortality\u0026#34;, component = \u0026#34;mortality_data\u0026#34;, dimension_names = list(location = rownames(array), date = colnames(array))) issue_with_component(index, handle, issue = \u0026#34;this data is bad\u0026#34;, severity = 7) finalise(handle) initialise() # responsible for reading the working config.yaml file\nregisters the working config.yaml file, submission script, and GitHub repo registers a CodeRun (since the CodeRun UUID should be referenced if ${{RUN_ID}} is specified in a DataProduct name) returns a handle containing: the working config.yaml file contents the object id for this file the object id for the submission script file the object id for the CodeRun Note that since, StorageLocation has a uniqueness constraint on path, hash, public, and storage_root, files with the same hash in the same storage_root and public flag should generate new Object entries that point to the same path. Likewise, files with the same hash in the same storage_root and public flag should not be duplicated in the data store.\nlink_read() # responsible for returning the path of an external object in the local data store\nupdates handle with file path (if not already present) and useful metadata returns file path read_array() # responsible for reading the correct data product, which at this point has been downloaded from the remote data store by fair pull\nreads a specified version of the data updates handle link_write() # updates handle with useful metadata returns a path to write to write_array() # responsible for writing an array as a component to an hdf5 file\nwrites component to the hdf5 file updates handle if this is the first component to be written, update handle with storage location if this is not the first component to be written, reference the storage location from the handle if the component is already recorded in the handle, return the index of this handle reference invisibly raise_issue() # responsible for writing issue related metadata to the handle\nrecords issue and data product / component related metadata in the handle note that where an issue is associated with an entire object, the whole_object component is referenced; the whole_object component is generated automatically whenever a new Object is created components or whole data products may be referenced by name or via a reference (for instance returned by write_array()) multiple components or data products may be linked to a single issue, either via two functions - one to raise issues and one to attach them - or via arguments to raise_issue() that allow multiple components to be attached finalise() # renames files with their hash until this point, we\u0026rsquo;ve arbitrarily named each file e.g. dat-{random_hash}.{extension} this is renamed as b03bbbe1205b3de70b1ae7573cf11c8b2555d2ed.{extension} renames data products if variables are present, e.g. for human/outbreak/simulation_run-${{RUN_ID}}, ${{RUN_ID}} is replaced with the CodeRun UUID records metadata (e.g. location, components, various descriptions, issues) in the data registry updates the code run in the data registry submission_script.py # Alternatively, the submission script may be written in Python.\nfrom data_pipeline_api.standard_api import StandardAPI with StandardAPI.from_config(\u0026#34;config.yaml\u0026#34;) as api: data = read(api.link_read(\u0026#34;records/SARS-CoV-2/scotland/cases-and-management\u0026#34;)) data api.write_array(\u0026#34;records/SARS-CoV-2/scotland/cases-and-management/mortality\u0026#34;, \u0026#34;mortality_data\u0026#34;, matrix) api.issue_with_component(\u0026#34;records/SARS-CoV-2/scotland/cases-and-management/mortality\u0026#34;, \u0026#34;mortality_data\u0026#34;, \u0026#34;this data is bad\u0026#34;, \u0026#34;7\u0026#34;) api.finalise() submission_script.jl # Alternatively, the submission script may be written in Julia.\nusing DataPipeline # Open the connection to the local registry with a given config file handle = initialise(\u0026#34;working-config.yaml\u0026#34;, \u0026#34;script.sh\u0026#34;) # Return location of file stored in the pipeline input_path = link_read(handle) # Process raw data and write data product data = read_csv(input_path) array = some_processing(data) # e.g. data wrangling, running a model, etc. index = write_estimate(array, handle, data_product = \u0026#34;records/SARS-CoV-2/scotland/cases-and-management/mortality\u0026#34;, component = \u0026#34;mortality_data\u0026#34;) issue_with_component(index, handle, issue = \u0026#34;this data is bad\u0026#34;, severity = 7) finalise(handle) C++ # Java # "
      }
      ,"33": {
        "url": "/docs/edit-site/",
        "relUrl": "/docs/edit-site/",
        "title": "Edit this site",
        "doc": "Edit this site",
        "section": "Docs",
        "content": " How to edit this site # If you want to edit or add to any of the pages here, please click on the \u0026ldquo;Edit this page\u0026rdquo; link at the bottom of the page you want to edit. This will take you to a GitHub repository, which stores the source files for that particular page. From there you can click on the tiny pencil (on hover: Edit this file), which will allow you to make any edits you want. Please remember that any tables or fancy formatting will have to be written in markdown / html to be visible. Once you\u0026rsquo;re finished, just click the link at the bottom of the page to create a new branch and start a pull request.\nIf you want to add new pages to the website remember that the headings are weighted (ordered) in the _index file. Sub headings are ordered within those directories and I\u0026rsquo;ve given them a different unit for clarity. Best to try clicking on everything when you\u0026rsquo;re done, to make sure they don\u0026rsquo;t randomly rearrange themselves.\n"
      }
      ,"34": {
        "url": "/",
        "relUrl": "/",
        "title": "Introduction",
        "doc": "Introduction",
        "section": "",
        "content": " Introduction # The FAIR Data Pipeline is intended to enable tracking of provenance of FAIR (findable, accessible, interoperable and reusable) data used in epidemiological modelling. Pipeline APIs written in C++, Java, Julia, Python and R can be called by modelling software for data ingestion. These interact with a local relational database storing metadata and the local filesystem, and are configured using a yaml file associated with the model run. Local files and metadata can be synchronised with a remote registry via a command line tool (fair).\nThe key benefits of using the FAIR Data Pipeline are:\nData recorded in a FAIR fashion (metadata on all data and code open and available for inspection) Provenance tracing allows model outputs to be traced to inputs and modelling code Multiple language support Designed to run on a broad range of platforms (including HPC, inside Safe Havens) Designed to be set up and completed online (to down-/up-load data) and run offline (Safe Havens will require this) Open metadata provides knowledge of or access to shared central data for specific domains (e.g. COVID-19 epidemiological modelling) Running Models # To use the FAIR Data Pipeline with a piece of modelling software, you must add a language specific Pipeline API as a dependency and interact with data registered in the pipeline via the methods it presents. Each model run must be configured using a config.yml file which specifies inputs and outputs by metadata.\ngraph LR; subgraph CLI fair end subgraph Local API API[Pipeline API] CY[config.yml] end subgraph localhost LR[Registry] FS[File Store] end subgraph Model MC[Model code] end fair --\u003e CY CY --\u003e API fair --\u003e MC MC --\u003e |read/write/link_*| API API --\u003e |read/write/link_*| LR LR --\u003e |read/link_*| API API --\u003e |read/link_*| MC API --\u003e |write_*| FS FS --\u003e |read_*| API MC --\u003e |\"(from link_write)\"| FS FS --\u003e |\"(from link_read)\"| MC Getting data # The command line utility fair is used to download and upload data and metadata required for and produced by model runs.\ngraph LR; subgraph Remote RR OS URI end subgraph Local LR FS end RR[Remote Registry]--\u003e|fair pull| LR[Local Registry] LR--\u003e|fair push| RR RR--\u003eOS(Managed Object Store) RR--\u003eURI(Arbitrary URI) LR--\u003eFS(Local Filesystem) OS--\u003e|fair pull| FS URI--\u003e|fair pull| FS FS--\u003e|fair push| OS FS--\u003e|fair push| URI "
      }}
  